# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.5'
services:
#
  spark-master:
    image: spark-py:3.1.2-latest
    build:
      context: .
      dockerfile: docker-spark/Dockerfile
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    env_file:
      - docker-spark/spark.env
    volumes:
      - /apps/hostpath:/apps/hostpath
    environment:
      NODE_TYPE: MASTER
      WEBUI_PORT: 8080

#
  spark-node01:
    image: spark-py:3.1.2-latest
    build:
      context: .
      dockerfile: docker-spark/Dockerfile
    container_name: spark-node01
    hostname: spark-node01
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    env_file:
      - docker-spark/spark.env
    volumes:
      - /apps/hostpath:/apps/hostpath
    environment:
      NODE_TYPE: WORKER
      WEBUI_PORT: 8081

#
  spark-node02:
    image: spark-py:3.1.2-latest
    build:
      context: .
      dockerfile: docker-spark/Dockerfile
    container_name: spark-node02
    hostname: spark-node02
    depends_on:
      - spark-master
    ports:
      - "8082:8082"
    env_file:
      - docker-spark/spark.env
    volumes:
      - /apps/hostpath:/apps/hostpath
    environment:
      NODE_TYPE: WORKER
      WEBUI_PORT: 8082

#
  history-server:
    image: spark-py:3.1.2-latest
    build:
      context: .
      dockerfile: docker-spark/Dockerfile
    container_name: history-server
    hostname: history-server
    depends_on:
      - spark-master
    ports:
      - "18080:18080"
    env_file:
      - docker-spark/spark.env
    volumes:
      - /apps/hostpath:/apps/hostpath
    environment:
      NODE_TYPE: LOGSERVER

#
networks:
  default:
    name: spark.net
