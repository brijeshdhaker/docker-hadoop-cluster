# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.5'

include:
  - dc-hadoop-cluster.yml

services:
  #
  #
  #
  spark-master:
    image: docker.io/brijeshdhaker/spark-standalon:3.1.2
    container_name: spark-master
    hostname: spark-master.sandbox.net
    ports:
      - "7077:7077"
      - "8080:8080"
    depends_on:
      datanode:
        condition: service_healthy
    healthcheck:
      test: nc -vz spark-master.sandbox.net 7077 || exit 1
      retries: 20
      interval: 10s
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive
    environment:
      - SPARK_MASTER_LOG=/apps/var/log/spark/spark-master.out
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_DRIVER_CORES=1
      - SPARK_DRIVER_MEMORY=1G
    env_file:
      - ./envs/docker_clients.env

  #
  #
  #
  spark-worker-a:
    image: docker.io/brijeshdhaker/spark-standalon:3.1.2
    container_name: spark-worker-a
    hostname: spark-worker-a.sandbox.net
    ports:
      - "8081:8081"
    healthcheck:
      test: nc -vz spark-worker-a.sandbox.net 8081 || exit 1
      retries: 20
      interval: 10s
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master.sandbox.net:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-a
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_LOG=/apps/var/log/spark/spark-worker-a.out
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - ./conf/kerberos/krb5.conf:/etc/krb5.conf
      - ./conf/hadoop/client:/opt/hadoop/etc/hadoop
      - ./conf/spark/conf:/opt/spark/conf
    env_file:
      - ./envs/docker_clients.env
  #
  #
  #
  spark-worker-b:
    image: docker.io/brijeshdhaker/spark-standalon:3.1.2
    container_name: spark-worker-b
    hostname: spark-worker-b.sandbox.net
    ports:
      - "8082:8082"
    healthcheck:
      test: nc -vz spark-worker-a.sandbox.net 8082 || exit 1
      retries: 20
      interval: 10s
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master.sandbox.net:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-b
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_WORKER_LOG=/apps/var/log/spark/spark-worker-b.out
    env_file:
      - ./envs/docker_clients.env
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - ./conf/kerberos/krb5.conf:/etc/krb5.conf
      - ./conf/hadoop/client:/opt/hadoop/etc/hadoop
      - ./conf/spark/conf:/opt/spark/conf

  #
  #
  #
  #
  # Spark History Server
  #
  sparkhistory:
    image: brijeshdhaker/spark-standalon:3.1.2
    container_name: sparkhistory
    hostname: sparkhistory.sandbox.net
    healthcheck:
      test: curl -f http://sparkhistory.sandbox.net:18080 || exit 1
      retries: 20
      interval: 10s
    environment:
      SPARK_WORKLOAD: HistoryServer
    ports:
      - "18080:18080"
    env_file:
      - ./envs/docker_clients.env
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_311:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - ./conf/kerberos/krb5.conf:/etc/krb5.conf
      - ./conf/hadoop/client:/opt/hadoop/etc/hadoop
      - ./conf/spark/conf:/opt/spark/conf

#
volumes:
  sandbox_apps_path:
    external: true
  #
  sandbox_hive_313:
    external: true

#
networks:
  default:
    external: true
    driver: bridge
    name: sandbox.net
