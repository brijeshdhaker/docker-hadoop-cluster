# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.5'

#
include:
  - path: ../bd-hadoop-sandbox/dc-minio.yml
  - path: ../bd-hadoop-sandbox/dc-mysqlserver.yml

#
services:
  #
  spark-master:
    image: brijeshdhaker/spark-notebook:3.5.1
    hostname: spark-master.sandbox.net
    container_name: spark-master
    command: [ spark-class org.apache.spark.deploy.master.Master --ip spark-master.sandbox.net --port 7077 --webui-port 8080 ]
    depends_on:
      - minio
      - mysqlserver
    healthcheck:
      test: curl -f http://spark-master.sandbox.net:8080 || exit 1
      retries: 20
      interval: 10s
    volumes:
      - sandbox_apps_path:/apps
      - ./conf/spark/conf:/opt/spark/conf
    environment:
      - AWS_ACCESS_KEY_ID=pgm2H2bR7a5kMc5XCYdO
      - AWS_SECRET_ACCESS_KEY=zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG
      - AWS_REGION=us-east-1
      - GRANT_SUDO=yes
      - PYARROW_IGNORE_TIMEZONE=1
      - SPARK_LOG_DIR=/apps/var/logs/spark/
    ports:
      - 7077:7077
      - 8080:8080

  #
  spark-worker-a:
    image: brijeshdhaker/spark-notebook:3.5.1
    hostname: spark-worker-a.sandbox.net
    container_name: spark-worker-a
    command: [ spark-class org.apache.spark.deploy.worker.Worker --host localhost --webui-port 8081 --cores 4 --memory 6G spark://spark-master:7077 ]
    depends_on:
      - spark-master
    healthcheck:
      test: curl -f http://spark-worker-a.sandbox.net:8081 || exit 1
      retries: 20
      interval: 10s
    volumes:
      - sandbox_apps_path:/apps
      - ./conf/spark/conf:/opt/spark/conf
    environment:
      - AWS_ACCESS_KEY_ID=pgm2H2bR7a5kMc5XCYdO
      - AWS_SECRET_ACCESS_KEY=zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG
      - AWS_REGION=us-east-1
      - GRANT_SUDO=yes
      - PYARROW_IGNORE_TIMEZONE=1
      - SPARK_LOG_DIR=/apps/var/logs/spark/
    ports:
      - 8081:8081

  #
  spark-worker-b:
    image: brijeshdhaker/spark-notebook:3.5.1
    hostname: spark-worker-b.sandbox.net
    container_name: spark-worker-b
    command: [ spark-class org.apache.spark.deploy.worker.Worker --host localhost --webui-port 8082 --cores 4 --memory 6G spark://spark-master:7077 ]
    depends_on:
      - spark-master
    healthcheck:
      test: curl -f http://spark-worker-b.sandbox.net:8082 || exit 1
      retries: 20
      interval: 10s
    volumes:
      - sandbox_apps_path:/apps
      - ./conf/spark/conf:/opt/spark/conf
    environment:
      - AWS_ACCESS_KEY_ID=pgm2H2bR7a5kMc5XCYdO
      - AWS_SECRET_ACCESS_KEY=zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG
      - AWS_REGION=us-east-1
      - GRANT_SUDO=yes
      - PYARROW_IGNORE_TIMEZONE=1
      - SPARK_LOG_DIR=/apps/var/logs/spark/
    ports:
      - 8082:8082

  #
  spark-history:
    image: brijeshdhaker/spark-notebook:3.5.1
    hostname: spark-history.sandbox.net
    container_name: spark-history
    command: [ spark-class org.apache.spark.deploy.history.HistoryServer --properties-file /opt/spark/conf/spark-history.conf ]
    depends_on:
      - spark-master
    healthcheck:
      test: curl -f http://spark-history.sandbox.net:18080 || exit 1
      retries: 20
      interval: 10s
    volumes:
      - sandbox_apps_path:/apps
      - ./conf/spark/conf:/opt/spark/conf
    environment:
      - AWS_ACCESS_KEY_ID=pgm2H2bR7a5kMc5XCYdO
      - AWS_SECRET_ACCESS_KEY=zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG
      - AWS_REGION=us-east-1
      - GRANT_SUDO=yes
      - PYARROW_IGNORE_TIMEZONE=1
      - SPARK_LOG_DIR=/apps/var/logs/spark/
    ports:
      - 18080:18080

  #
  spark-notebook:
    image: brijeshdhaker/spark-notebook:3.5.1
    hostname: spark-notebook.sandbox.net
    container_name: spark-notebook
    depends_on:
      - spark-master
    healthcheck:
      test: curl -f http://spark-notebook.sandbox.net:8888 || exit 1
      retries: 20
      interval: 10s
    volumes:
      - sandbox_apps_path:/apps
      - /apps/sandbox/warehouse:/home/iceberg/warehouse
      - /apps/sandbox/notebooks:/home/iceberg/notebooks/notebooks
      - ./conf/spark/conf:/opt/spark/conf
    environment:
      - AWS_ACCESS_KEY_ID=pgm2H2bR7a5kMc5XCYdO
      - AWS_SECRET_ACCESS_KEY=zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG
      - AWS_REGION=us-east-1
      - GRANT_SUDO=yes
      - PYARROW_IGNORE_TIMEZONE=1
    ports:
      - 4040:4040
      - 8888:8888

#
volumes:
  sandbox_apps_path:
    external: true

#
networks:
  default:
    external: true
    driver: bridge
    name: sandbox.net
