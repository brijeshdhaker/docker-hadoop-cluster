{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "sparkConf = SparkConf() \\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[4]\").\n",
    "        appName('Sample Spark Application').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "## Load data from csv\n",
    "##\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "#employee_df.printSchema()\n",
    "\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "## 1 Job = 1 Stage = 1 Task      For Read Operation\n",
    "## 1 Job = 1 Stage = 1 Task      For inferSchema Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec19fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c0d1fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e89551a634e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Load data from csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "##\n",
    "## spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "## Load data from csv\n",
    "##\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "#employee_df.printSchema()\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()\n",
    "\n",
    "\n",
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764d72c",
   "metadata": {},
   "source": [
    "#### Q-001. If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "`Adaptive Query Execution (AQE)`: In Spark 3.0 and later, Adaptive Query Execution (AQE) is an optimization that can dynamically adjust the number of shuffle partitions during runtime. AQE can coalesce small shuffle partitions into larger ones, effectively reducing the number of tasks if the data distribution allows for it. If AQE is enabled and determines that fewer partitions are optimal, you will see fewer than 200 tasks.\n",
    "\n",
    "`Data Volume and Distribution`: If the amount of data being processed is very small, or if the data is highly skewed (meaning a few keys have a disproportionately large amount of data), Spark might not utilize all 200 partitions efficiently, or AQE might optimize the partition count.\n",
    "\n",
    "`Coalesce or Repartition before GroupBy`: If a coalesce or repartition operation was performed on the DataFrame immediately before the groupBy, it could explicitly set the number of partitions, overriding the default shuffle partition setting for that specific operation.\n",
    "\n",
    "`Specific Optimization Strategies`: Certain optimization strategies or custom partitioning schemes might be in place that influence the number of partitions used during the groupBy operation, leading to a task count different from 200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5396070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1,6)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6780d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.select((df.id).alias(\"##\"), col(\"id\").alias(\"#ID\") ,(df.id + 10).alias('###COL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8968a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"id * 5\").alias(\"##\") , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"id * 5\" , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = spark.createDataFrame(data=[(1000, 'Nick'), (1001, 'John'), (1002, 'Frank')], schema=['id', 'name'])\n",
    "names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71e1e1",
   "metadata": {},
   "source": [
    "## Hello\n",
    ">This is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Sum of the first 100 whole numbers\n",
    "rdd = sc.parallelize(range(100 + 1))\n",
    "rdd.sum()\n",
    "# 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86485073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-sql in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: pymysql in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: prettytable in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (3.16.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (9.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=2.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (2.0.43)\n",
      "Requirement already satisfied: sqlparse in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (0.5.3)\n",
      "Requirement already satisfied: six in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (1.17.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from sqlalchemy>=2.0->ipython-sql) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from sqlalchemy>=2.0->ipython-sql) (4.15.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->ipython-sql) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from stack_data->ipython->ipython-sql) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from stack_data->ipython->ipython-sql) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from stack_data->ipython->ipython-sql) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipython-sql pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab28cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426f47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql mysql+pymysql://mysqladmin:mysqladmin@mysqlserver.sandbox.net:3306/NEETASTUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "159ffc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://mysqladmin:***@mysqlserver.sandbox.net:3306/NEETASTUDIO\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql USE NEETASTUDIO;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819a5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9d44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://mysqladmin:***@mysqlserver.sandbox.net:3306/NEETASTUDIO\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>ID</th>\n",
       "            <th>NAME</th>\n",
       "            <th>EMAIL</th>\n",
       "            <th>PHONE</th>\n",
       "            <th>ENQUIRY_FOR</th>\n",
       "            <th>MESSAGE</th>\n",
       "            <th>ADD_TS</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>Deepika D.</td>\n",
       "            <td>deepikadhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>Swati R.</td>\n",
       "            <td>swatir@gmail.com</td>\n",
       "            <td>+91 9820937448</td>\n",
       "            <td>kids</td>\n",
       "            <td>I am looking for kid photography</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>Tejas D.</td>\n",
       "            <td>tejasdhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 'Deepika D.', 'deepikadhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (2, 'Swati R.', 'swatir@gmail.com', '+91 9820937448', 'kids', 'I am looking for kid photography', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (3, 'Tejas D.', 'tejasdhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql select * from CUSTOMER_ENQUIRIES LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d3b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://mysqladmin:***@mysqlserver.sandbox.net:3306/NEETASTUDIO\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>ID</th>\n",
       "            <th>NAME</th>\n",
       "            <th>EMAIL</th>\n",
       "            <th>PHONE</th>\n",
       "            <th>ENQUIRY_FOR</th>\n",
       "            <th>MESSAGE</th>\n",
       "            <th>ADD_TS</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>Deepika D.</td>\n",
       "            <td>deepikadhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>Swati R.</td>\n",
       "            <td>swatir@gmail.com</td>\n",
       "            <td>+91 9820937448</td>\n",
       "            <td>kids</td>\n",
       "            <td>I am looking for kid photography</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>Tejas D.</td>\n",
       "            <td>tejasdhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 'Deepika D.', 'deepikadhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (2, 'Swati R.', 'swatir@gmail.com', '+91 9820937448', 'kids', 'I am looking for kid photography', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (3, 'Tejas D.', 'tejasdhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    " \n",
    "select * from CUSTOMER_ENQUIRIES LIMIT 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
