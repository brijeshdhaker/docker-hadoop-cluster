{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5e4906-f2be-457f-ba00-ddde8a74013a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/30 15:39:12 WARN Utils: Your hostname, vmware-ubuntu-24.04 resolves to a loopback address: 127.0.1.1; using 192.168.154.133 instead (on interface ens33)\n",
      "25/09/30 15:39:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/brijeshdhaker/.ivy2/cache\n",
      "The jars for the packages stored in: /home/brijeshdhaker/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c4d6c647-d4e4-4307-a0e3-afed99c94e08;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.0.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.199 in central\n",
      ":: resolution report :: resolve 208ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.199 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c4d6c647-d4e4-4307-a0e3-afed99c94e08\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n",
      "25/09/30 15:39:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.154.133:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-sql-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x73b7f170c290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0\")\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-sql-notebook').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e26cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee966cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromCSV = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"delimiter\", ',') \\\n",
    "    .options(inferSchema='True', delimiter=',') \\\n",
    "    .csv(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "dfFromCSV.printSchema()\n",
    "dfFromCSV.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b06979",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromJSON = spark.read.json(\"file:///apps/sandbox/defaultfs/people.json\")\n",
    "dfFromJSON.printSchema()\n",
    "dfFromJSON.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3558adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(employee_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6992dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|emp_id|emp_name|emp_role |emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|7521  |WARD    |SALESMAN |7698       |1982-06-23  |1250      |500     |30      |\n",
      "|7788  |SCOTT   |ANALYST  |7566       |1984-12-17  |3000      |NULL    |20      |\n",
      "|7654  |MARTIN  |SALESMAN |7698       |1981-07-21  |1250      |1400    |30      |\n",
      "|7935  |ROBERT  |ANALYST  |7566       |1984-03-31  |1940      |225     |10      |\n",
      "|7839  |KING    |PRESIDENT|NULL       |1981-11-17  |5000      |NULL    |10      |\n",
      "|7566  |JONES   |MANAGER  |7839       |1981-02-04  |2975      |345     |20      |\n",
      "|7844  |TURNER  |SALESMAN |7698       |1982-09-08  |1500      |0       |30      |\n",
      "|7876  |ADAMS   |CLERK    |7788       |1980-11-14  |1100      |100     |20      |\n",
      "|7782  |CLARK   |MANAGER  |7839       |1983-09-06  |2450      |NULL    |10      |\n",
      "|7900  |JAMES   |CLERK    |7788       |1983-08-05  |950       |NULL    |30      |\n",
      "|7499  |ALLEN   |SALESMAN |7698       |1981-05-05  |1600      |300     |30      |\n",
      "|7934  |MILLER  |CLERK    |7788       |1982-01-02  |1300      |225     |10      |\n",
      "|7369  |SMITH   |CLERK    |7902       |1980-04-04  |800       |NULL    |20      |\n",
      "|7902  |FORD    |ANALYST  |7566       |1981-11-27  |3000      |NULL    |20      |\n",
      "|7698  |BLAKE   |MANAGER  |7839       |1981-01-05  |2850      |125     |30      |\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "#\n",
    "employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3b1e5",
   "metadata": {},
   "source": [
    "## Process Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeabe4e",
   "metadata": {},
   "source": [
    "### Map Dataframe Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|emp_id|emp_name|emp_role |emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|7521  |WARD    |SALESMAN |7698       |1982-06-23  |3750      |750     |30      |\n",
      "|7788  |SCOTT   |ANALYST  |7566       |1984-12-17  |4000      |100     |20      |\n",
      "|7654  |MARTIN  |SALESMAN |7698       |1981-07-21  |3750      |1650    |30      |\n",
      "|7935  |ROBERT  |ANALYST  |7566       |1984-03-31  |2940      |325     |10      |\n",
      "|7839  |KING    |PRESIDENT|NULL       |1981-11-17  |5000      |0       |10      |\n",
      "|7566  |JONES   |MANAGER  |7839       |1981-02-04  |4975      |545     |20      |\n",
      "|7844  |TURNER  |SALESMAN |7698       |1982-09-08  |4000      |250     |30      |\n",
      "|7876  |ADAMS   |CLERK    |7788       |1980-11-14  |2600      |250     |20      |\n",
      "|7782  |CLARK   |MANAGER  |7839       |1983-09-06  |4450      |200     |10      |\n",
      "|7900  |JAMES   |CLERK    |7788       |1983-08-05  |2450      |150     |30      |\n",
      "|7499  |ALLEN   |SALESMAN |7698       |1981-05-05  |4100      |550     |30      |\n",
      "|7934  |MILLER  |CLERK    |7788       |1982-01-02  |2800      |375     |10      |\n",
      "|7369  |SMITH   |CLERK    |7902       |1980-04-04  |2300      |150     |20      |\n",
      "|7902  |FORD    |ANALYST  |7566       |1981-11-27  |4000      |100     |20      |\n",
      "|7698  |BLAKE   |MANAGER  |7839       |1981-01-05  |4850      |325     |30      |\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def applyMap(row):\n",
    "    bonus = 0\n",
    "    salary = 0\n",
    "\n",
    "    if row.emp_comm is None :\n",
    "        bonus = 0\n",
    "    else :\n",
    "        bonus = row.emp_comm\n",
    "\n",
    "\n",
    "    if row.emp_role == 'ANALYST':\n",
    "        salary = row.emp_salary + 1000\n",
    "        bonus = bonus + 100\n",
    "    elif row.emp_role == 'CLERK':\n",
    "        salary = row.emp_salary + 1500\n",
    "        bonus = bonus + 150\n",
    "    elif row.emp_role == 'MANAGER':\n",
    "        salary = row.emp_salary + 2000\n",
    "        bonus = bonus + 200\n",
    "    elif row.emp_role == 'SALESMAN':\n",
    "        salary = row.emp_salary + 2500\n",
    "        bonus = bonus + 250\n",
    "    else:\n",
    "        salary = row.emp_salary\n",
    "        bonus = 0\n",
    "    \n",
    "    return (row.emp_id, row.emp_name, row.emp_role, row.emp_manager, row.emp_hiredate, salary, bonus, row.emp_dept)\n",
    "\n",
    "\n",
    "df1_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "\n",
    "##\n",
    "#df1 = employee_df.rdd.map(lambda r: (r.emp_id, r.emp_name, r.emp_role, r.emp_manager, r.emp_hiredate, r.emp_salary, 225, r.emp_dept)).toDF(df1_columns)\n",
    "\n",
    "##\n",
    "df1 = employee_df.rdd.map(applyMap).toDF(df1_columns)\n",
    "df1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da3b66",
   "metadata": {},
   "source": [
    "### flatMap Dataframe Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39df0a8b",
   "metadata": {},
   "source": [
    "### Process Dataframe Partitions\n",
    "Unfortunately, PySpark DataFame doesnâ€™t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column. Below is a complete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53628d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  NULL|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96a9c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            sentence|  words|\n",
      "+--------------------+-------+\n",
      "|      Spark is great|  Spark|\n",
      "|      Spark is great|     is|\n",
      "|      Spark is great|  great|\n",
      "|Map and FlatMap a...|    Map|\n",
      "|Map and FlatMap a...|    and|\n",
      "|Map and FlatMap a...|FlatMap|\n",
      "|Map and FlatMap a...|    are|\n",
      "|Map and FlatMap a...| useful|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Spark is great\",), (\"Map and FlatMap are useful\",)]\n",
    "df = spark.createDataFrame(data, [\"sentence\"])\n",
    "\n",
    "# Use 'explode' function to achieve flatMap-like behavior\n",
    "df_words = df.withColumn(\"words\", explode(split(col(\"sentence\"), \" \")))\n",
    "\n",
    "df_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25dec3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|emp_id|emp_name|emp_role |emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|7521  |WARD    |SALESMAN |7698       |1982-06-23  |1250      |500     |30      |\n",
      "|7788  |SCOTT   |ANALYST  |7566       |1984-12-17  |3000      |50      |20      |\n",
      "|7654  |MARTIN  |SALESMAN |7698       |1981-07-21  |1250      |1400    |30      |\n",
      "|7935  |ROBERT  |ANALYST  |7566       |1984-03-31  |1940      |225     |10      |\n",
      "|7839  |KING    |PRESIDENT|NULL       |1981-11-17  |5000      |50      |10      |\n",
      "|7566  |JONES   |MANAGER  |7839       |1981-02-04  |2975      |345     |20      |\n",
      "|7844  |TURNER  |SALESMAN |7698       |1982-09-08  |1500      |0       |30      |\n",
      "|7876  |ADAMS   |CLERK    |7788       |1980-11-14  |1100      |100     |20      |\n",
      "|7782  |CLARK   |MANAGER  |7839       |1983-09-06  |2450      |50      |10      |\n",
      "|7900  |JAMES   |CLERK    |7788       |1983-08-05  |9500      |50      |30      |\n",
      "|7499  |ALLEN   |SALESMAN |7698       |1981-05-05  |1600      |300     |30      |\n",
      "|7934  |MILLER  |CLERK    |7788       |1982-01-02  |1300      |225     |10      |\n",
      "|7369  |SMITH   |CLERK    |7902       |1980-04-04  |8000      |50      |20      |\n",
      "|7902  |FORD    |ANALYST  |7566       |1981-11-27  |3000      |50      |20      |\n",
      "|7698  |BLAKE   |MANAGER  |7839       |1981-01-05  |2850      |125     |30      |\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using mapPartitions with yield\n",
    "def formatWithYield(partition_data):\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None :\n",
    "            bonus = 50\n",
    "        yield (record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df2 = employee_df.rdd.mapPartitions(formatWithYield).toDF(df2_columns)\n",
    "df2.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dafa379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|emp_id|emp_name|emp_role |emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|7521  |WARD    |SALESMAN |7698       |1982-06-23  |1250      |500     |30      |\n",
      "|7788  |SCOTT   |ANALYST  |7566       |1984-12-17  |3000      |50      |20      |\n",
      "|7654  |MARTIN  |SALESMAN |7698       |1981-07-21  |1250      |1400    |30      |\n",
      "|7935  |ROBERT  |ANALYST  |7566       |1984-03-31  |1940      |225     |10      |\n",
      "|7839  |KING    |PRESIDENT|NULL       |1981-11-17  |5000      |50      |10      |\n",
      "|7566  |JONES   |MANAGER  |7839       |1981-02-04  |2975      |345     |20      |\n",
      "|7844  |TURNER  |SALESMAN |7698       |1982-09-08  |1500      |0       |30      |\n",
      "|7876  |ADAMS   |CLERK    |7788       |1980-11-14  |1100      |100     |20      |\n",
      "|7782  |CLARK   |MANAGER  |7839       |1983-09-06  |2450      |50      |10      |\n",
      "|7900  |JAMES   |CLERK    |7788       |1983-08-05  |9500      |50      |30      |\n",
      "|7499  |ALLEN   |SALESMAN |7698       |1981-05-05  |1600      |300     |30      |\n",
      "|7934  |MILLER  |CLERK    |7788       |1982-01-02  |1300      |225     |10      |\n",
      "|7369  |SMITH   |CLERK    |7902       |1980-04-04  |8000      |50      |20      |\n",
      "|7902  |FORD    |ANALYST  |7566       |1981-11-27  |3000      |50      |20      |\n",
      "|7698  |BLAKE   |MANAGER  |7839       |1981-01-05  |2850      |125     |30      |\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using mapPartitions with iterator\n",
    "def formatWithIter(partition_data):\n",
    "    p_data = []\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        p_data.append([record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept])\n",
    "    return iter(p_data)\n",
    "\n",
    "df3_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df3 = employee_df.rdd.mapPartitions(formatWithIter).toDF(df3_columns)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d641d",
   "metadata": {},
   "source": [
    "### Process Dataframe with mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "432cacb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+--------+--------+-----------+------------+----------+--------+--------+\n",
      "|partition|emp_id|emp_name|emp_role|emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|\n",
      "+---------+------+--------+--------+-----------+------------+----------+--------+--------+\n",
      "|1        |7521  |WARD    |MANAGER |7698       |1982-06-23  |1250      |500     |30      |\n",
      "|1        |7788  |SCOTT   |MANAGER |7566       |1984-12-17  |3000      |50      |20      |\n",
      "|1        |7654  |MARTIN  |MANAGER |7698       |1981-07-21  |1250      |1400    |30      |\n",
      "|1        |7935  |ROBERT  |MANAGER |7566       |1984-03-31  |1940      |225     |10      |\n",
      "|1        |7839  |KING    |MANAGER |NULL       |1981-11-17  |5000      |50      |10      |\n",
      "|1        |7566  |JONES   |MANAGER |7839       |1981-02-04  |2975      |345     |20      |\n",
      "|1        |7844  |TURNER  |MANAGER |7698       |1982-09-08  |1500      |0       |30      |\n",
      "|2        |7876  |ADAMS   |ANALYST |7788       |1980-11-14  |1100      |100     |20      |\n",
      "|2        |7782  |CLARK   |MANAGER |7839       |1983-09-06  |2450      |50      |10      |\n",
      "|2        |7900  |JAMES   |ANALYST |7788       |1983-08-05  |9500      |50      |30      |\n",
      "|2        |7499  |ALLEN   |MANAGER |7698       |1981-05-05  |1600      |300     |30      |\n",
      "|2        |7934  |MILLER  |ANALYST |7788       |1982-01-02  |1300      |225     |10      |\n",
      "|2        |7369  |SMITH   |ANALYST |7902       |1980-04-04  |8000      |50      |20      |\n",
      "|2        |7902  |FORD    |MANAGER |7566       |1981-11-27  |3000      |50      |20      |\n",
      "|2        |7698  |BLAKE   |MANAGER |7839       |1981-01-05  |2850      |125     |30      |\n",
      "+---------+------+--------+--------+-----------+------------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "def formatWithMapPartitionsWithIndex(partitionIndex, iterator):\n",
    "    for paartion_data in iterator:\n",
    "        role = 'ANALYST' if  paartion_data.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = paartion_data.emp_salary\n",
    "        if paartion_data.emp_salary <= 1000 :\n",
    "            salary = paartion_data.emp_salary * 10\n",
    "        \n",
    "        bonus = paartion_data.emp_comm \n",
    "        if paartion_data.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        yield (partitionIndex+1, paartion_data.emp_id, paartion_data.emp_name, role, paartion_data.emp_manager, paartion_data.emp_hiredate, salary, bonus, paartion_data.emp_dept)\n",
    "    \n",
    "    #yield (partitionIndex, len(list(iterator)))\n",
    "    \n",
    "\n",
    "df4_columns = [\"partition\",\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df4 = employee_df.rdd.mapPartitionsWithIndex(formatWithMapPartitionsWithIndex).toDF(df4_columns)\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ce37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ecc44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27ecc1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(emp_dept=20, count=3),\n",
       " Row(emp_dept=10, count=2),\n",
       " Row(emp_dept=30, count=1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11050f0b-4572-4d66-84ea-60024986f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1,6)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a1c39a6-a610-464b-ae68-3de4d585689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2243a53-c2e7-4d25-bca1-9daddb1d4517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "| ##|#ID|###COL|\n",
      "+---+---+------+\n",
      "|  1|  1|    11|\n",
      "|  2|  2|    12|\n",
      "|  3|  3|    13|\n",
      "|  4|  4|    14|\n",
      "|  5|  5|    15|\n",
      "+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select((df.id).alias(\"##\"), col(\"id\").alias(\"#ID\") ,(df.id + 10).alias('###COL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cb7d037-2901-46aa-8139-547f0ee601cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|(id * 5)| id|\n",
      "+--------+---+\n",
      "|       5|  1|\n",
      "|      10|  2|\n",
      "|      15|  3|\n",
      "|      20|  4|\n",
      "|      25|  5|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"id * 5\" , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d159539-be14-4be5-bb07-98c7523d5057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = spark.createDataFrame(data=[(1000, 'Nick'), (1001, 'John'), (1002, 'Frank')], schema=['id', 'name'])\n",
    "names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d65fa97-41c0-4478-8fbc-65b0dcbc3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  id| name|\n",
      "+----+-----+\n",
      "|1000| Nick|\n",
      "|1001| John|\n",
      "|1002|Frank|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dce81-e47d-4c0e-8493-9245de4f149b",
   "metadata": {},
   "source": [
    "#### Q-001. If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "`Adaptive Query Execution (AQE)`: In Spark 3.0 and later, Adaptive Query Execution (AQE) is an optimization that can dynamically adjust the number of shuffle partitions during runtime. AQE can coalesce small shuffle partitions into larger ones, effectively reducing the number of tasks if the data distribution allows for it. If AQE is enabled and determines that fewer partitions are optimal, you will see fewer than 200 tasks.\n",
    "\n",
    "`Data Volume and Distribution`: If the amount of data being processed is very small, or if the data is highly skewed (meaning a few keys have a disproportionately large amount of data), Spark might not utilize all 200 partitions efficiently, or AQE might optimize the partition count.\n",
    "\n",
    "`Coalesce or Repartition before GroupBy`: If a coalesce or repartition operation was performed on the DataFrame immediately before the groupBy, it could explicitly set the number of partitions, overriding the default shuffle partition setting for that specific operation.\n",
    "\n",
    "`Specific Optimization Strategies`: Certain optimization strategies or custom partitioning schemes might be in place that influence the number of partitions used during the groupBy operation, leading to a task count different from 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afd6189f-5c5b-4c83-97a2-5440840c4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "|1000|\n",
      "|1001|\n",
      "|1002|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55ba8911-988b-488c-85d8-644557dc0208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Sum of the first 100 whole numbers\n",
    "rdd = sc.parallelize(range(100 + 1))\n",
    "rdd.sum()\n",
    "# 5050"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
