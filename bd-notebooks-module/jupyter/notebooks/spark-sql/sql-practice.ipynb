{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c71f87",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e329a04",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-sql-notebook').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33638a0c",
   "metadata": {},
   "source": [
    "### Spark Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d002b8f",
   "metadata": {},
   "source": [
    "#### Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc556031",
   "metadata": {},
   "source": [
    "#### Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859db39",
   "metadata": {},
   "source": [
    "### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0396804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\", \"TX\":\"Texas\", \"CH\":\"Chicago\"} \n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "print(\"{}\".format(broadcastStates.value))\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)\n",
    "\n",
    "## Broadcast variable on filter\n",
    "filteDf= df.where((df['state'].isin(list(broadcastStates.value.keys()))))\n",
    "filteDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea47e3e",
   "metadata": {},
   "source": [
    "### Accumulators Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf89ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Accumulator\n",
    "sc_acc = spark.sparkContext.accumulator(0)\n",
    "print(\"Accumulator initial value: {}\".format(sc_acc.value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9a9be",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## By default, Spark's CSV reader treats empty strings (\"\") and blank values (e.g., ,, where nothing is between the commas) as null\n",
    "\n",
    "# This code will read \"N/A\" as null\n",
    "dfFromCSV = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"delimiter\", ',') \\\n",
    "    .option(\"emptyValue\", 'N/A') \\\n",
    "    .option(\"nullValue\", 0) \\\n",
    "    .option(\"treatEmptyValuesAsNulls\", True) \\\n",
    "    .options(inferSchema=True, delimiter=',') \\\n",
    "    .csv(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "\n",
    "#dfFromCSV.printSchema()\n",
    "dfFromCSV.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromJSON = spark.read.json(\"file:///apps/sandbox/defaultfs/cdrs.json\")\n",
    "dfFromJSON.printSchema()\n",
    "dfFromJSON.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(employee_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "\n",
    "#df_with_schema.printSchema()\n",
    "\n",
    "df_with_schema.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67679c3a",
   "metadata": {},
   "source": [
    "### Handeling Null & Empyty Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Replacing null values with some value\n",
    "#\n",
    "df_with_schema.fillna(value=0).show()\n",
    "df_with_schema.fillna(value=0,subset=[\"emp_comm\"]).show() # only for emp_comm column\n",
    "df_with_schema.fillna(10,[\"emp_comm\"]) \\\n",
    "    .fillna(\"---\",[\"emp_manager\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Replacing null values with some value using na\n",
    "#\n",
    "df_with_schema.na.fill(value=0).show()\n",
    "df_with_schema.na.fill(value=0,subset=[\"emp_comm\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employee_columns = ['emp_id', 'emp_name', 'emp_role', 'emp_manager', 'emp_hiredate', 'emp_salary', 'emp_comm', 'emp_dept']\n",
    "\n",
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    schema=employee_schema\n",
    ")\n",
    "\n",
    "#\n",
    "# employee_df.printSchema()\n",
    "\n",
    "#\n",
    "# print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dept_columns = ['dept_id', 'dept_name', 'dept_location']\n",
    "\n",
    "dept_schema = StructType() \\\n",
    "    .add(\"dept_id\", IntegerType(), True) \\\n",
    "    .add(\"dept_name\", StringType(), True) \\\n",
    "    .add(\"dept_location\", StringType(), True)\n",
    "\n",
    "dept_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(dept_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/departments.csv\")\n",
    "\n",
    "#dept_df.printSchema()\n",
    "\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d11b66",
   "metadata": {},
   "source": [
    "### Data Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "employee_df = employee_df.repartition(2)\n",
    "\n",
    "#\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "# employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7a374",
   "metadata": {},
   "source": [
    "## Process Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82b525",
   "metadata": {},
   "source": [
    "### Map Dataframe Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50663e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def applyMap(row):\n",
    "    bonus = 0\n",
    "    salary = 0\n",
    "\n",
    "    if row.emp_comm is None :\n",
    "        bonus = 0\n",
    "    else :\n",
    "        bonus = row.emp_comm\n",
    "\n",
    "\n",
    "    if row.emp_role == 'ANALYST':\n",
    "        salary = row.emp_salary + 1000\n",
    "        bonus = bonus + 100\n",
    "    elif row.emp_role == 'CLERK':\n",
    "        salary = row.emp_salary + 1500\n",
    "        bonus = bonus + 150\n",
    "    elif row.emp_role == 'MANAGER':\n",
    "        salary = row.emp_salary + 2000\n",
    "        bonus = bonus + 200\n",
    "    elif row.emp_role == 'SALESMAN':\n",
    "        salary = row.emp_salary + 2500\n",
    "        bonus = bonus + 250\n",
    "    else:\n",
    "        salary = row.emp_salary\n",
    "        bonus = 0\n",
    "    \n",
    "    return (row.emp_id, row.emp_name, row.emp_role, row.emp_manager, row.emp_hiredate, salary, bonus, row.emp_dept)\n",
    "\n",
    "\n",
    "df1_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "\n",
    "##\n",
    "#df1 = employee_df.rdd.map(lambda r: (r.emp_id, r.emp_name, r.emp_role, r.emp_manager, r.emp_hiredate, r.emp_salary, 225, r.emp_dept)).toDF(df1_columns)\n",
    "\n",
    "##\n",
    "df1 = employee_df.rdd.map(applyMap).toDF(df1_columns)\n",
    "df1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cfaec",
   "metadata": {},
   "source": [
    "### flatMap Dataframe Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd64d6d",
   "metadata": {},
   "source": [
    "### Process Dataframe Partitions\n",
    "Unfortunately, PySpark DataFame doesnâ€™t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column. Below is a complete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a04123",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Spark is great\",), (\"Map and FlatMap are useful\",)]\n",
    "df = spark.createDataFrame(data, [\"sentence\"])\n",
    "\n",
    "# Use 'explode' function to achieve flatMap-like behavior\n",
    "df_words = df.withColumn(\"words\", explode(split(col(\"sentence\"), \" \")))\n",
    "\n",
    "df_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mapPartitions with yield\n",
    "def formatWithYield(partition_data):\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None :\n",
    "            bonus = 50\n",
    "        yield (record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df2 = employee_df.rdd.mapPartitions(formatWithYield).toDF(df2_columns)\n",
    "df2.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab38b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mapPartitions with iterator\n",
    "def formatWithIter(partition_data):\n",
    "    p_data = []\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        p_data.append([record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept])\n",
    "    return iter(p_data)\n",
    "\n",
    "df3_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df3 = employee_df.rdd.mapPartitions(formatWithIter).toDF(df3_columns)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38280f01",
   "metadata": {},
   "source": [
    "### Process Dataframe with mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "def formatWithMapPartitionsWithIndex(partitionIndex, iterator):\n",
    "    for paartion_data in iterator:\n",
    "        role = 'ANALYST' if  paartion_data.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = paartion_data.emp_salary\n",
    "        if paartion_data.emp_salary <= 1000 :\n",
    "            salary = paartion_data.emp_salary * 10\n",
    "        \n",
    "        bonus = paartion_data.emp_comm \n",
    "        if paartion_data.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        yield (partitionIndex+1, paartion_data.emp_id, paartion_data.emp_name, role, paartion_data.emp_manager, paartion_data.emp_hiredate, salary, bonus, paartion_data.emp_dept)\n",
    "    \n",
    "    #yield (partitionIndex, len(list(iterator)))\n",
    "    \n",
    "\n",
    "df4_columns = [\"partition\",\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df4 = employee_df.rdd.mapPartitionsWithIndex(formatWithMapPartitionsWithIndex).toDF(df4_columns)\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1,6)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select((df.id).alias(\"##\"), col(\"id\").alias(\"#ID\") ,(df.id + 10).alias('###COL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"id * 5\" , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = spark.createDataFrame(data=[(1000, 'Nick'), (1001, 'John'), (1002, 'Frank')], schema=['id', 'name'])\n",
    "names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ba19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3477967",
   "metadata": {},
   "source": [
    "#### Q-001. If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "`Adaptive Query Execution (AQE)`: In Spark 3.0 and later, Adaptive Query Execution (AQE) is an optimization that can dynamically adjust the number of shuffle partitions during runtime. AQE can coalesce small shuffle partitions into larger ones, effectively reducing the number of tasks if the data distribution allows for it. If AQE is enabled and determines that fewer partitions are optimal, you will see fewer than 200 tasks.\n",
    "\n",
    "`Data Volume and Distribution`: If the amount of data being processed is very small, or if the data is highly skewed (meaning a few keys have a disproportionately large amount of data), Spark might not utilize all 200 partitions efficiently, or AQE might optimize the partition count.\n",
    "\n",
    "`Coalesce or Repartition before GroupBy`: If a coalesce or repartition operation was performed on the DataFrame immediately before the groupBy, it could explicitly set the number of partitions, overriding the default shuffle partition setting for that specific operation.\n",
    "\n",
    "`Specific Optimization Strategies`: Certain optimization strategies or custom partitioning schemes might be in place that influence the number of partitions used during the groupBy operation, leading to a task count different from 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550893",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Sum of the first 100 whole numbers\n",
    "rdd = sc.parallelize(range(100 + 1))\n",
    "rdd.sum()\n",
    "# 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65902c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "## Load data from csv\n",
    "##\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "#employee_df.printSchema()\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()\n",
    "\n",
    "\n",
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036caa97",
   "metadata": {},
   "source": [
    "#### Get highest salary of each group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get highest salary of each group  \n",
    "w3 = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"row\",row_number().over(w3)) \\\n",
    "  .filter(col(\"row\") == 1).drop(\"row\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7936b75",
   "metadata": {},
   "source": [
    "#### Get max, min, avg, sum of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edab149",
   "metadata": {},
   "outputs": [],
   "source": [
    "w4 = Window.partitionBy(\"department\")\n",
    "df.withColumn(\"row\",row_number().over(w3)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(w4)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
