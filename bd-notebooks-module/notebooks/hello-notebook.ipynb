{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e909c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for e1 in range(1,10) : print(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfc7615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "arr = [e1 for e1 in range(1,10)]\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7c899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/13 19:11:25 WARN Utils: Your hostname, vmware-ubuntu-24.04 resolves to a loopback address: 127.0.1.1; using 192.168.154.133 instead (on interface ens33)\n",
      "25/10/13 19:11:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/13 19:11:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.154.133:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Sample Spark Application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7037fba101d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "sparkConf = SparkConf() \\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[4]\").\n",
    "        appName('Sample Spark Application').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e431457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723f6611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "## Load data from csv\n",
    "##\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "#employee_df.printSchema()\n",
    "\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "## 1 Job = 1 Stage = 1 Task      For Read Operation\n",
    "## 1 Job = 1 Stage = 1 Task      For inferSchema Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92970b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[MISSING_GROUP_BY] The query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.;\nAggregate [emp_dept#24, sum(emp_salary#22) AS total#69L]\n+- Relation [emp_id#17,emp_name#18,emp_role#19,emp_manager#20,emp_hiredate#21,emp_salary#22,emp_comm#23,emp_dept#24] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43memployee_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memp_dept\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memp_salary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.groupby(\u001b[33m\"\u001b[39m\u001b[33memp_dept\u001b[39m\u001b[33m\"\u001b[39m).show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3229\u001b[39m, in \u001b[36mDataFrame.select\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m   3184\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   3185\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   3186\u001b[39m \n\u001b[32m   3187\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3227\u001b[39m \u001b[33;03m    +-----+---+\u001b[39;00m\n\u001b[32m   3228\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3229\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [MISSING_GROUP_BY] The query does not include a GROUP BY clause. Add GROUP BY or turn it into the window functions using OVER clauses.;\nAggregate [emp_dept#24, sum(emp_salary#22) AS total#69L]\n+- Relation [emp_id#17,emp_name#18,emp_role#19,emp_manager#20,emp_hiredate#21,emp_salary#22,emp_comm#23,emp_dept#24] csv\n"
     ]
    }
   ],
   "source": [
    "employee_df.groupby(\"emp_dept\").sum(\"emp_salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec19fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c0d1fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e89551a634e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Load data from csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "##\n",
    "## spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "## Load data from csv\n",
    "##\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "#employee_df.printSchema()\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()\n",
    "\n",
    "\n",
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764d72c",
   "metadata": {},
   "source": [
    "#### Q-001. If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "`Adaptive Query Execution (AQE)`: In Spark 3.0 and later, Adaptive Query Execution (AQE) is an optimization that can dynamically adjust the number of shuffle partitions during runtime. AQE can coalesce small shuffle partitions into larger ones, effectively reducing the number of tasks if the data distribution allows for it. If AQE is enabled and determines that fewer partitions are optimal, you will see fewer than 200 tasks.\n",
    "\n",
    "`Data Volume and Distribution`: If the amount of data being processed is very small, or if the data is highly skewed (meaning a few keys have a disproportionately large amount of data), Spark might not utilize all 200 partitions efficiently, or AQE might optimize the partition count.\n",
    "\n",
    "`Coalesce or Repartition before GroupBy`: If a coalesce or repartition operation was performed on the DataFrame immediately before the groupBy, it could explicitly set the number of partitions, overriding the default shuffle partition setting for that specific operation.\n",
    "\n",
    "`Specific Optimization Strategies`: Certain optimization strategies or custom partitioning schemes might be in place that influence the number of partitions used during the groupBy operation, leading to a task count different from 200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5396070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1,6)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6780d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.select((df.id).alias(\"##\"), col(\"id\").alias(\"#ID\") ,(df.id + 10).alias('###COL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8968a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"id * 5\").alias(\"##\") , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"id * 5\" , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = spark.createDataFrame(data=[(1000, 'Nick'), (1001, 'John'), (1002, 'Frank')], schema=['id', 'name'])\n",
    "names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71e1e1",
   "metadata": {},
   "source": [
    "## Hello\n",
    ">This is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Sum of the first 100 whole numbers\n",
    "rdd = sc.parallelize(range(100 + 1))\n",
    "rdd.sum()\n",
    "# 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86485073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-sql in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (0.5.0)\n",
      "Requirement already satisfied: pymysql in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: prettytable in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (3.16.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (9.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=2.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (2.0.43)\n",
      "Requirement already satisfied: sqlparse in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (0.5.3)\n",
      "Requirement already satisfied: six in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (1.17.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from sqlalchemy>=2.0->ipython-sql) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from sqlalchemy>=2.0->ipython-sql) (4.15.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from ipython->ipython-sql) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->ipython-sql) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from jedi>=0.16->ipython->ipython-sql) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from stack_data->ipython->ipython-sql) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from stack_data->ipython->ipython-sql) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages (from stack_data->ipython->ipython-sql) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipython-sql pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab28cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426f47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql mysql+pymysql://mysqladmin:mysqladmin@mysqlserver.sandbox.net:3306/NEETASTUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "159ffc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://mysqladmin:***@mysqlserver.sandbox.net:3306/NEETASTUDIO\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql USE NEETASTUDIO;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819a5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9d44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://mysqladmin:***@mysqlserver.sandbox.net:3306/NEETASTUDIO\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>ID</th>\n",
       "            <th>NAME</th>\n",
       "            <th>EMAIL</th>\n",
       "            <th>PHONE</th>\n",
       "            <th>ENQUIRY_FOR</th>\n",
       "            <th>MESSAGE</th>\n",
       "            <th>ADD_TS</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>Deepika D.</td>\n",
       "            <td>deepikadhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>Swati R.</td>\n",
       "            <td>swatir@gmail.com</td>\n",
       "            <td>+91 9820937448</td>\n",
       "            <td>kids</td>\n",
       "            <td>I am looking for kid photography</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>Tejas D.</td>\n",
       "            <td>tejasdhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 'Deepika D.', 'deepikadhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (2, 'Swati R.', 'swatir@gmail.com', '+91 9820937448', 'kids', 'I am looking for kid photography', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (3, 'Tejas D.', 'tejasdhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql select * from CUSTOMER_ENQUIRIES LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d3b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * mysql+pymysql://mysqladmin:***@mysqlserver.sandbox.net:3306/NEETASTUDIO\n",
      "3 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>ID</th>\n",
       "            <th>NAME</th>\n",
       "            <th>EMAIL</th>\n",
       "            <th>PHONE</th>\n",
       "            <th>ENQUIRY_FOR</th>\n",
       "            <th>MESSAGE</th>\n",
       "            <th>ADD_TS</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>Deepika D.</td>\n",
       "            <td>deepikadhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>Swati R.</td>\n",
       "            <td>swatir@gmail.com</td>\n",
       "            <td>+91 9820937448</td>\n",
       "            <td>kids</td>\n",
       "            <td>I am looking for kid photography</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3</td>\n",
       "            <td>Tejas D.</td>\n",
       "            <td>tejasdhaker@gmail.com</td>\n",
       "            <td>+91 9820937445</td>\n",
       "            <td>maternity</td>\n",
       "            <td>I am looking for maternity photoshoot</td>\n",
       "            <td>2025-09-30 04:57:33</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, 'Deepika D.', 'deepikadhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (2, 'Swati R.', 'swatir@gmail.com', '+91 9820937448', 'kids', 'I am looking for kid photography', datetime.datetime(2025, 9, 30, 4, 57, 33)),\n",
       " (3, 'Tejas D.', 'tejasdhaker@gmail.com', '+91 9820937445', 'maternity', 'I am looking for maternity photoshoot', datetime.datetime(2025, 9, 30, 4, 57, 33))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    " \n",
    "select * from CUSTOMER_ENQUIRIES LIMIT 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
