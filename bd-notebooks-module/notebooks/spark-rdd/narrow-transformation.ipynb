{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.ui.port\", \"4042\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    #.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    #.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "sc = SparkContext(master='local[*]', appName=\"narrow-transformations\", conf=sparkConf)\n",
    "sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82803de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Learn\", \"Apache\", \"Spark\", \"Learn\", \"Spark\", \"RDD\", \"Functions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_1 = sc.parallelize(data, 8)\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "print(\"RDD-1 Record Count : %i \" % (rdd_1.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b8f0a",
   "metadata": {},
   "source": [
    "#### RDD.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = range(1, 101)\n",
    "rdd_1 = spark.sparkContext.parallelize(data, 2)\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "print(\"Values in RDD-1 : {0} \".format(rdd_1.collect()))\n",
    "\n",
    "\n",
    "rdd_2 = rdd_1.map(lambda x: (x, x*2))\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_2.getNumPartitions()))\n",
    "print(\"Values in RDD-2 : {0} \".format(rdd_2.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b2d47",
   "metadata": {},
   "source": [
    "#### RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-partitioning RDD\n",
    "rdd_2 = rdd_1.repartition(4)\n",
    "print(\"RDD-2 Partition count after re-partitions is  : %i \" % (rdd_2.getNumPartitions()))\n",
    "print(rdd_2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Reduce Partition Count\n",
    "rdd_2 = rdd_1.coalesce(4)\n",
    "print(\"RDD-2 Partition Count : %i \" % (rdd_2.getNumPartitions()))\n",
    "print(\"RDD-2 Record Count : %i \" % (rdd_2.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2 = rdd_1.distinct()\n",
    "print(\"RDD-2 Partition Count : %i \" % (rdd_2.getNumPartitions()))\n",
    "print(\"RDD-2 Record Count : %i \" % (rdd_2.count()))\n",
    "\n",
    "# print\n",
    "for e in rdd_2.collect(): print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d278ed9",
   "metadata": {},
   "source": [
    "#### RDD.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Kenith\", \"Marketing\", \"CA\", 66000, 36, 40000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000),\n",
    "    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Shelly\", \"Marketing\", \"NY\", 60000, 15, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "def m_filter(element):\n",
    "    return element[4] > 30\n",
    "\n",
    "rdd_1 = sc.parallelize(data, 8)\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "print(\"RDD-1 Record Count : %i \" % (rdd_1.count()))\n",
    "\n",
    "\n",
    "\n",
    "rdd_2 = rdd_1.filter(lambda x: x[4] > 30)\n",
    "print(rdd_2.collect())\n",
    "print(\"RDD-2 Partition Count : %i \" % (rdd_2.getNumPartitions()))\n",
    "print(\"RDD-2 Record Count : %i \" % (rdd_2.count()))\n",
    "\n",
    "rdd_3 = rdd_1.filter(m_filter)\n",
    "print(rdd_3.collect())\n",
    "print(\"RDD-3 Partition Count : %i \" % (rdd_3.getNumPartitions()))\n",
    "print(\"RDD-3 Record Count : %i \" % (rdd_3.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfa973",
   "metadata": {},
   "source": [
    "#### RDD.flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, ['A', 'B', 'C']),\n",
    "    (2, ['D', 'E', 'F']),\n",
    "    (3, ['G', 'H', 'I']),\n",
    "    (4, ['J', 'K', 'L']),\n",
    "    (5, ['M', 'N', 'O']),\n",
    "    (6, ['P', 'Q', 'R']),\n",
    "    (7, ['S', 'T', 'U']),\n",
    "    (8, ['V', 'W', 'X']),\n",
    "    (9, ['Y', 'Z', 'A']),\n",
    "    (10, ['B', 'C', 'D'])\n",
    "]\n",
    "\n",
    "def flatMapFun(x):\n",
    "    l = []\n",
    "    for e in x[1]:\n",
    "        l.append((x[0], e))\n",
    "    return l\n",
    "\n",
    "rdd_1 = spark.sparkContext.parallelize(data, 2)\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "print(\"RDD-1 Record Count : %i \" % (rdd_1.count()))\n",
    "print(rdd_1.collect())\n",
    "\n",
    "rdd_2 = rdd_1.flatMap(flatMapFun, False)\n",
    "print(\"RDD-2 Partition Count : %i \" % (rdd_2.getNumPartitions()))\n",
    "print(\"RDD-2 Record Count : %i \" % (rdd_2.count()))\n",
    "print(rdd_2.collect())\n",
    "\n",
    "rdd_3 = rdd_1.flatMap(lambda x: (x[0], x[0]**2, x[0]*10))\n",
    "print(\"RDD-3 Partition Count : %i \" % (rdd_3.getNumPartitions()))\n",
    "print(\"RDD-3 Record Count : %i \" % (rdd_3.count()))\n",
    "print(rdd_3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22833e50",
   "metadata": {},
   "source": [
    "#### RDD.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import re\n",
    "\n",
    "COMMA_DELIMITER = ',(?=([^\"]*\"[^\"]*\")*[^\"]*$)'\n",
    "\n",
    "def myFun(x):\n",
    "    splits = re.split(r\",\", x)\n",
    "    return splits[1]+\", \"+splits[6]\n",
    "\n",
    "def myFilter(x):\n",
    "    f = re.split(r\",(?![^(]*?\\))\\s*\", x)[6]\n",
    "    try:\n",
    "        y = float(f)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    else:\n",
    "        if y > 40:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"spark-rdd\")\n",
    "\n",
    "rdd_1 = sc.textFile(\"file:///apps/hostpath/datasets/airports.text\")\n",
    "ps = rdd_1.getNumPartitions()\n",
    "print(\"Partition Size : \" + str(ps))\n",
    "# rdd_2 = rdd_1.map(lambda x: x.replace('\"', ''))\n",
    "rdd_3 = rdd_1.filter(lambda x: myFilter(x))\n",
    "rdd_4 = rdd_3.map(lambda x: myFun(x))\n",
    "for e in rdd_4.collect(): print(e)\n",
    "\n",
    "total = rdd_4.count()\n",
    "print(total)\n",
    "sc.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a9aaf",
   "metadata": {},
   "source": [
    "#### RDD.mapPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatWithYield(partition_data):\n",
    "    for record in partition_data:\n",
    "        sex = 'Female' if  record.sex == 'F' else 'Male'\n",
    "        bonus = record.salary * 10/100\n",
    "        yield record.first + \" \" + record.last, sex, record.salary, bonus\n",
    "\n",
    "def formatWithIter(partition_data):\n",
    "    p_data = []\n",
    "    for record in partition_data:\n",
    "        sex = 'Female' if  record.sex == 'F' else 'Male'\n",
    "        bonus = record.salary * 10/100\n",
    "        p_data.append([record.first + \" \" + record.last, sex, record.salary, bonus])\n",
    "    return iter(p_data)\n",
    "\n",
    "data = [\n",
    "        ('Brijesh', 'Dhaker', 'M', 10000),\n",
    "        ('Neeta', 'Dhakad', 'F', 70000),\n",
    "        ('Keshvi', 'Dhaker', 'F', 20000),\n",
    "        ('Tejas', 'Kumar', 'M', 45000),\n",
    "        ('Sunil', 'Gupta', 'M', 10000)\n",
    "]\n",
    "columns = ['first', 'last', 'sex', 'salary']\n",
    "\n",
    "df1 = spark.createDataFrame(data=data, schema=columns)\n",
    "df1.printSchema()\n",
    "\n",
    "print(\"DF-1 Partition Count : %i \" % (df1.rdd.getNumPartitions()))\n",
    "# print(rdd_1.glom().collect())\n",
    "\n",
    "commons.print_separator()\n",
    "\n",
    "df2 = df1.rdd.mapPartitions(formatWithYield).toDF(['fullname', 'sex', 'salary', 'bonus'])\n",
    "print(\"DF-2 Partition count after transformation is : %i \" % (df2.rdd.getNumPartitions()))\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "commons.print_separator()\n",
    "df3 = df1.rdd.mapPartitions(formatWithIter).toDF(['fullname', 'sex', 'salary', 'bonus'])\n",
    "print(\"DF-3 Partition count after transformation is : %i \" % (df3.rdd.getNumPartitions()))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00944792",
   "metadata": {},
   "source": [
    "#### RDD.mapPartitionsWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "def f(partitionIndex, iterator):\n",
    "    yield (partitionIndex, len(list(iterator)))\n",
    "\n",
    "#    data = range(1, 100)\n",
    "rdd_1 = sc.parallelize(list(range(1, 100)), 4)\n",
    "print(\"Records Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "\n",
    "rdd_2 = rdd_1.mapPartitionsWithIndex(f)\n",
    "print(rdd_2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d3d475",
   "metadata": {},
   "source": [
    "#### RDD.union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e157a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [\n",
    "    (\"James\", \"Sales\", \"NY\", 90000, 34, 10000),\n",
    "    (\"Michael\", \"Sales\", \"NY\", 86000, 56, 20000),\n",
    "    (\"Robert\", \"Sales\", \"CA\", 81000, 30, 23000),\n",
    "    (\"Maria\", \"Finance\", \"CA\", 90000, 24, 23000),\n",
    "    (\"Raman\", \"Finance\", \"CA\", 99000, 40, 24000)\n",
    "]\n",
    "\n",
    "data_2 = [\n",
    "    (\"Scott\", \"Finance\", \"NY\", 83000, 36, 19000),\n",
    "    (\"Jen\", \"Finance\", \"NY\", 79000, 53, 15000),\n",
    "    (\"Jeff\", \"Marketing\", \"CA\", 80000, 25, 18000),\n",
    "    (\"Kumar\", \"Marketing\", \"NY\", 91000, 50, 21000)\n",
    "]\n",
    "\n",
    "rdd_1 = sc.parallelize(data_1)\n",
    "print(\"RDD-1 Record Count : %i \" % (rdd_1.count()))\n",
    "\n",
    "rdd_2 = sc.parallelize(data_2)\n",
    "print(\"RDD-2 Record Count : %i \" % (rdd_2.count()))\n",
    "\n",
    "rdd_3 = rdd_1.union(rdd_2)\n",
    "print(rdd_3.collect())\n",
    "print(\"RDD-3 Record Count : %i \" % (rdd_3.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff3729",
   "metadata": {},
   "source": [
    "#### RDD.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7382c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(range(1, 1000001))\n",
    "rdd_1 = sc.parallelize(data)\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "#print(rdd_1.collect())\n",
    "\n",
    "rdd_2 = rdd_1.sample(False, 0.002)\n",
    "print(\"RDD-2 Partition count after re-partitions is  : %i \" % (rdd_2.getNumPartitions()))\n",
    "print(rdd_2.collect())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
