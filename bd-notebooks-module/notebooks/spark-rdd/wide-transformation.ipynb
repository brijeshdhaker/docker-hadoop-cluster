{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a72d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.ui.port\", \"4042\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    #.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    #.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "sc = SparkContext(master='local[*]', appName=\"wide-transformations\", conf=sparkConf)\n",
    "sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_marks = [\n",
    "    (\"S-001\",\"Hindi\",456),(\"S-001\",\"English\",567),(\"S-001\",\"Math\",345),(\"S-001\",\"Science\",987),\n",
    "    (\"S-002\",\"Hindi\",878),(\"S-002\",\"English\",767),(\"S-002\",\"Math\",456),(\"S-002\",\"Science\",567),\n",
    "    (\"S-003\",\"Hindi\",630),(\"S-003\",\"English\",889),(\"S-003\",\"Math\",765),(\"S-003\",\"Science\",879),\n",
    "    (\"S-004\",\"Hindi\",879),(\"S-004\",\"English\",965),(\"S-004\",\"Math\",876),(\"S-004\",\"Science\",666),\n",
    "    (\"S-005\",\"Hindi\",567),(\"S-005\",\"English\",907),(\"S-005\",\"Math\",645),(\"S-005\",\"Science\",876),\n",
    "    (\"S-006\",\"Hindi\",776),(\"S-006\",\"English\",534),(\"S-006\",\"Math\",768),(\"S-006\",\"Science\",565),\n",
    "    (\"S-007\",\"Hindi\",556),(\"S-007\",\"English\",630),(\"S-007\",\"Math\",568),(\"S-007\",\"Science\",896),\n",
    "    (\"S-008\",\"Hindi\",765),(\"S-008\",\"English\",765),(\"S-008\",\"Math\",987),(\"S-008\",\"Science\",990),\n",
    "    (\"S-009\",\"Hindi\",777),(\"S-009\",\"English\",657),(\"S-009\",\"Math\",887),(\"S-009\",\"Science\",768),\n",
    "    (\"S-010\",\"Hindi\",648),(\"S-010\",\"English\",878),(\"S-010\",\"Math\",768),(\"S-010\",\"Science\",458),\n",
    "    (\"S-011\",\"Hindi\",878),(\"S-011\",\"English\",567),(\"S-011\",\"Math\",598),(\"S-011\",\"Science\",788),\n",
    "    (\"S-012\",\"Hindi\",779),(\"S-012\",\"English\",765),(\"S-012\",\"Math\",776),(\"S-012\",\"Science\",887),\n",
    "    (\"S-013\",\"Hindi\",887),(\"S-013\",\"English\",771),(\"S-013\",\"Math\",987),(\"S-013\",\"Science\",768),\n",
    "    (\"S-014\",\"Hindi\",677),(\"S-014\",\"English\",662),(\"S-014\",\"Math\",666),(\"S-014\",\"Science\",776),\n",
    "    (\"S-015\",\"Hindi\",550),(\"S-015\",\"English\",559),(\"S-015\",\"Math\",768),(\"S-015\",\"Science\",565),\n",
    "    (\"S-016\",\"Hindi\",625),(\"S-016\",\"English\",989),(\"S-016\",\"Math\",660),(\"S-016\",\"Science\",556)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c41116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import portable_hash\n",
    "from random import random\n",
    "import hashlib\n",
    "\n",
    "keys = [\"S-001\",\"S-002\",\"S-003\",\"S-004\",\"S-005\",\"S-006\",\"S-007\",\"S-008\",\"S-009\",\"S-010\",\"S-011\",\"S-012\",\"S-013\",\"S-014\",\"S-015\",\"S-016\"]\n",
    "\n",
    "def hash_partitioning(key):\n",
    "    return portable_hash(key) % len(keys)\n",
    "    \n",
    "    \n",
    "def hash_partitioning(key):\n",
    "    return hash(key) % len(keys)\n",
    "\n",
    "\n",
    "def key_partitioning(key):\n",
    "    return keys.index(key)\n",
    "\n",
    "\n",
    "hashValue = lambda str: portable_hash(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "def print_partitions(rdd):\n",
    "    numPartitions = rdd.getNumPartitions()\n",
    "    print(\"Total partitions: {}\".format(numPartitions))\n",
    "    print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "    parts = rdd.glom().collect()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in parts:\n",
    "        print(\"================ Partition - {} ================\".format(i))\n",
    "        for r in p:\n",
    "            print(\"Row {} : {} : {}\".format(j, r, (hash(r[0])%numPartitions)))\n",
    "            j = j+1\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff539fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "rawRDD = sc.parallelize(student_marks)\n",
    "marksRDD = rawRDD.map(lambda x: (x[0], (x[1], x[2]))).partitionBy(4, key_partitioning)\n",
    "\n",
    "# print(\"RDD-1 Partition Count : %i \" % (rawRDD.getNumPartitions()))\n",
    "# print(\"Values in RDD-1 : {0} \".format(rawRDD.glom().collect()))\n",
    "print_partitions(marksRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984bc26",
   "metadata": {},
   "source": [
    "#### RDD.reduceByKey : Student's Total Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "print(marksRDD.map(lambda x: (x[0], x[1][1])).reduceByKey(add).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90effcf1",
   "metadata": {},
   "source": [
    "#### RDD.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3566b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(marksRDD.groupByKey().glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2876b",
   "metadata": {},
   "source": [
    "#### combineByKey\n",
    "\n",
    "RDD.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb231334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "def to_list(a):\n",
    "    return [a]\n",
    "\n",
    "def append(a, b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "\n",
    "def extend(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "sorted(rdd.combineByKey(to_list, append, extend).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d96c7e",
   "metadata": {},
   "source": [
    "#### aggregateByKey\n",
    "RDD.aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "\n",
    "# Defining Sequential Operation and Combiner Operations\n",
    "# Sequence operation for aggregation\n",
    "def seq_agg(accumulator, element):\n",
    "    return (accumulator[0]+ element, accumulator[1] + 1)\n",
    "\n",
    "\n",
    "# Reduce/Combine operation for max aggregation#\n",
    "def comb_agg(accumulator1, accumulator2):\n",
    "    return (accumulator1[0] + accumulator2[0],accumulator1[1] + accumulator2[1])\n",
    "\n",
    "seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "sorted(rdd.aggregateByKey((0, 0), seq_agg, comb_agg).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a8859",
   "metadata": {},
   "source": [
    "#### Find Maximume for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e13b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2),(\"a\", 3), (\"b\", 14), (\"a\", 12)])\n",
    "\n",
    "# Defining Sequential Operation and Combiner Operations\n",
    "# Sequence operation for aggregation\n",
    "def seq_agg(accumulator, element):\n",
    "    min_val = min(accumulator[\"min\"], element)\n",
    "    max_val = max(accumulator[\"max\"], element)\n",
    "    return {\"min\":min_val, \"max\":max_val}\n",
    "\n",
    "\n",
    "# Reduce/Combine operation for max aggregation#\n",
    "def comb_agg(accumulator1, accumulator2):\n",
    "    min_val = min(accumulator1[\"min\"], accumulator2[\"min\"])\n",
    "    max_val = max(accumulator1[\"max\"], accumulator2[\"max\"])\n",
    "    return {\"min\":min_val, \"max\":max_val}\n",
    "\n",
    "print(rdd.aggregateByKey({\"min\":0, \"max\":0}, seq_agg, comb_agg).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd14565",
   "metadata": {},
   "source": [
    "#### RDD.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e16690",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_key_values = [\n",
    "    (\"ABC\", 10.5),\n",
    "    (\"XYZ\", 23.8),\n",
    "    (\"ABC\", 11.2),\n",
    "    (\"XYZ\", 20.3),\n",
    "    (\"ABC\", 10.7),\n",
    "    (\"XYZ\", 25.9),\n",
    "    (\"ABC\", 15.1),\n",
    "    (\"PQR\", 12.7),\n",
    "    (\"ABC\", 12.5),\n",
    "    (\"PQR\", 10.3),\n",
    "    (\"IJK\", 12.5),\n",
    "    (\"LMN\", 15.6)\n",
    "]\n",
    "\n",
    "rdd_1 = sc.parallelize(t_key_values)\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "print(\"Values in RDD-1 : {0} \".format(rdd_1.collect()))\n",
    "\n",
    "commons.print_separator()\n",
    "\n",
    "#\n",
    "rdd_2 = rdd_1.sortByKey(True)\n",
    "print(\"Values in RDD-2 : {0} \".format(rdd_2.collect()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
