{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/10 10:50:07 WARN Utils: Your hostname, vmware-ubuntu-24.04 resolves to a loopback address: 127.0.1.1; using 192.168.154.133 instead (on interface ens33)\n",
      "25/10/10 10:50:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/10 10:50:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.154.133:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-joins-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x74e7114d9c90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.ui.port\", \"4042\")\n",
    "    #.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    #.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    #.set(\"spark.hadoop.fs.defaultFS\", \"s3a://defaultfs/\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-joins-stratgies').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03469d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a List Containing the \"Column Names\" for \"Person\"\n",
    "personColumns = [\"Id\", \"First_Name\", \"Last_Name\", \"AddressId\"]\n",
    "\n",
    "# Create a List Containing the \"Data\" for \"Person\"\n",
    "personList = [\\\n",
    "                (1001, \"Oindrila\", \"Chakraborty\", \"A001\"),\n",
    "                (1002, \"Soumyajyoti\", \"Bagchi\", \"A002\"),\n",
    "                (1003, \"Oishi\", \"Bhattacharyya\", \"A004\"),\n",
    "                (1004, \"Sabarni\", \"Chakraborty\", \"A003\"),\n",
    "                (1005, \"Ayan\", \"Dutta\", \"A002\"),\n",
    "                (1006, \"Dhrubajyoti\", \"Das\", \"A004\"),\n",
    "                (1007, \"Sayantan\", \"Chatterjee\", \"A004\"),\n",
    "                (1008, \"Ritwik\", \"Ghosh\", \"A001\"),\n",
    "                (1009, \"Puja\", \"Bhatt\", \"A001\"),\n",
    "                (1010, \"Souvik\", \"Roy\", \"A002\"),\n",
    "                (1011, \"Souvik\", \"Roy\", \"A003\"),\n",
    "                (1012, \"Ria\", \"Ghosh\", \"A003\"),\n",
    "                (1013, \"Soumyajit\", \"Pal\", \"A002\"),\n",
    "                (1014, \"Abhirup\", \"Chakraborty\", \"A004\"),\n",
    "                (1015, \"Sagarneel\", \"Sarkar\", \"A003\"),\n",
    "                (1016, \"Anamika\", \"Pal\", \"A002\"),\n",
    "                (1017, \"Swaralipi\", \"Roy\", \"A002\"),\n",
    "                (1018, \"Rahul\", \"Roychowdhury\", \"A003\"),\n",
    "                (1019, \"Paulomi\", \"Mondal\", \"A004\"),\n",
    "                (1020, \"Avishek\", \"Basu\", \"A002\"),\n",
    "                (1021, \"Avirupa\", \"Ghosh\", \"A004\"),\n",
    "                (1022, \"Ameer\", \"Sengupta\", \"A003\"),\n",
    "                (1023, \"Megha\", \"Kargupta\", \"A002\"),\n",
    "                (1024, \"Madhura\", \"Chakraborty\", \"A002\"),\n",
    "                (1025, \"Debankur\", \"Dutta\", \"A002\"),\n",
    "                (1026, \"Bidisha\", \"Das\", \"A001\"),\n",
    "                (1027, \"Rohan\", \"Ghosh\", \"A004\"),\n",
    "                (1028, \"Tathagata\", \"Acharyya\", \"A003\")\n",
    "              ]\n",
    "\n",
    "# Create a List Containing the \"Column Names\" for \"Address\"\n",
    "addressColumns = [\"AddressId\", \"Address\"]\n",
    "\n",
    "# Create a List Containing the \"Data\" for \"Address\"\n",
    "addressList = [\\\n",
    "                  (\"A001\", \"India\"),\n",
    "                  (\"A002\", \"US\"),\n",
    "                  (\"A003\", \"UK\"),\n",
    "                  (\"A004\", \"UAE\")\n",
    "              ]\n",
    "\n",
    "###\n",
    "def display(df):\n",
    "    df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e600252",
   "metadata": {},
   "source": [
    "#### Create a DataFrame for “Person”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7674ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- First_Name: string (nullable = true)\n",
      " |-- Last_Name: string (nullable = true)\n",
      " |-- AddressId: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------+---------+\n",
      "|Id  |First_Name |Last_Name    |AddressId|\n",
      "+----+-----------+-------------+---------+\n",
      "|1001|Oindrila   |Chakraborty  |A001     |\n",
      "|1002|Soumyajyoti|Bagchi       |A002     |\n",
      "|1003|Oishi      |Bhattacharyya|A004     |\n",
      "|1004|Sabarni    |Chakraborty  |A003     |\n",
      "|1005|Ayan       |Dutta        |A002     |\n",
      "|1006|Dhrubajyoti|Das          |A004     |\n",
      "|1007|Sayantan   |Chatterjee   |A004     |\n",
      "|1008|Ritwik     |Ghosh        |A001     |\n",
      "|1009|Puja       |Bhatt        |A001     |\n",
      "|1010|Souvik     |Roy          |A002     |\n",
      "|1011|Souvik     |Roy          |A003     |\n",
      "|1012|Ria        |Ghosh        |A003     |\n",
      "|1013|Soumyajit  |Pal          |A002     |\n",
      "|1014|Abhirup    |Chakraborty  |A004     |\n",
      "|1015|Sagarneel  |Sarkar       |A003     |\n",
      "|1016|Anamika    |Pal          |A002     |\n",
      "|1017|Swaralipi  |Roy          |A002     |\n",
      "|1018|Rahul      |Roychowdhury |A003     |\n",
      "|1019|Paulomi    |Mondal       |A004     |\n",
      "|1020|Avishek    |Basu         |A002     |\n",
      "+----+-----------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPerson = spark.createDataFrame(personList, schema = personColumns)\n",
    "dfPerson.printSchema()\n",
    "display(dfPerson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f813883",
   "metadata": {},
   "source": [
    "#### Create Another DataFrame for “Address”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8d6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AddressId: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|AddressId|Address|\n",
      "+---------+-------+\n",
      "|A001     |India  |\n",
      "|A002     |US     |\n",
      "|A003     |UK     |\n",
      "|A004     |UAE    |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfAddress = spark.createDataFrame(addressList, schema = addressColumns)\n",
    "dfAddress.printSchema()\n",
    "display(dfAddress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4951a7cd",
   "metadata": {},
   "source": [
    "#### 1. Shuffle Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68012019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- First_Name: string (nullable = true)\n",
      " |-- Last_Name: string (nullable = true)\n",
      " |-- AddressId: string (nullable = true)\n",
      " |-- AddressId: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      "\n",
      "+----+-----------+------------+---------+---------+-------+\n",
      "|Id  |First_Name |Last_Name   |AddressId|AddressId|Address|\n",
      "+----+-----------+------------+---------+---------+-------+\n",
      "|1001|Oindrila   |Chakraborty |A001     |A001     |India  |\n",
      "|1008|Ritwik     |Ghosh       |A001     |A001     |India  |\n",
      "|1009|Puja       |Bhatt       |A001     |A001     |India  |\n",
      "|1026|Bidisha    |Das         |A001     |A001     |India  |\n",
      "|1002|Soumyajyoti|Bagchi      |A002     |A002     |US     |\n",
      "|1005|Ayan       |Dutta       |A002     |A002     |US     |\n",
      "|1010|Souvik     |Roy         |A002     |A002     |US     |\n",
      "|1013|Soumyajit  |Pal         |A002     |A002     |US     |\n",
      "|1016|Anamika    |Pal         |A002     |A002     |US     |\n",
      "|1017|Swaralipi  |Roy         |A002     |A002     |US     |\n",
      "|1020|Avishek    |Basu        |A002     |A002     |US     |\n",
      "|1023|Megha      |Kargupta    |A002     |A002     |US     |\n",
      "|1024|Madhura    |Chakraborty |A002     |A002     |US     |\n",
      "|1025|Debankur   |Dutta       |A002     |A002     |US     |\n",
      "|1004|Sabarni    |Chakraborty |A003     |A003     |UK     |\n",
      "|1011|Souvik     |Roy         |A003     |A003     |UK     |\n",
      "|1012|Ria        |Ghosh       |A003     |A003     |UK     |\n",
      "|1015|Sagarneel  |Sarkar      |A003     |A003     |UK     |\n",
      "|1018|Rahul      |Roychowdhury|A003     |A003     |UK     |\n",
      "|1022|Ameer      |Sengupta    |A003     |A003     |UK     |\n",
      "+----+-----------+------------+---------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPersonWithAddress = dfPerson.join(dfAddress, dfPerson.AddressId == dfAddress.AddressId, \"inner\")\n",
    "dfPersonWithAddress.printSchema()\n",
    "display(dfPersonWithAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf92d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [AddressId#3], [AddressId#25], Inner\n",
      "   :- Sort [AddressId#3 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(AddressId#3, 200), ENSURE_REQUIREMENTS, [plan_id=160]\n",
      "   :     +- Filter isnotnull(AddressId#3)\n",
      "   :        +- Scan ExistingRDD[Id#0L,First_Name#1,Last_Name#2,AddressId#3]\n",
      "   +- Sort [AddressId#25 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(AddressId#25, 200), ENSURE_REQUIREMENTS, [plan_id=161]\n",
      "         +- Filter isnotnull(AddressId#25)\n",
      "            +- Scan ExistingRDD[AddressId#25,Address#26]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPersonWithAddress.explain()\n",
    "\n",
    "## In the “Physical Plan”, it can be seen that the “Join” performed is “Sort Merge Join” indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c69f6",
   "metadata": {},
   "source": [
    "#### “Disable” the “Sort Merge Join” as the “Default Join Selection Strategy”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124d8f2",
   "metadata": {},
   "source": [
    "#### 2. Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa419433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Broadcast Hash Join by setting the threshold to -1\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# “Disable” the “Sort Merge Join” as the “Default Join Selection Strategy”.\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62cf3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- First_Name: string (nullable = true)\n",
      " |-- Last_Name: string (nullable = true)\n",
      " |-- AddressId: string (nullable = true)\n",
      " |-- AddressId: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      "\n",
      "+----+-----------+-------------+---------+---------+-------+\n",
      "|Id  |First_Name |Last_Name    |AddressId|AddressId|Address|\n",
      "+----+-----------+-------------+---------+---------+-------+\n",
      "|1003|Oishi      |Bhattacharyya|A004     |A004     |UAE    |\n",
      "|1002|Soumyajyoti|Bagchi       |A002     |A002     |US     |\n",
      "|1001|Oindrila   |Chakraborty  |A001     |A001     |India  |\n",
      "|1004|Sabarni    |Chakraborty  |A003     |A003     |UK     |\n",
      "|1006|Dhrubajyoti|Das          |A004     |A004     |UAE    |\n",
      "|1005|Ayan       |Dutta        |A002     |A002     |US     |\n",
      "|1007|Sayantan   |Chatterjee   |A004     |A004     |UAE    |\n",
      "|1008|Ritwik     |Ghosh        |A001     |A001     |India  |\n",
      "|1009|Puja       |Bhatt        |A001     |A001     |India  |\n",
      "|1011|Souvik     |Roy          |A003     |A003     |UK     |\n",
      "|1012|Ria        |Ghosh        |A003     |A003     |UK     |\n",
      "|1015|Sagarneel  |Sarkar       |A003     |A003     |UK     |\n",
      "|1014|Abhirup    |Chakraborty  |A004     |A004     |UAE    |\n",
      "|1010|Souvik     |Roy          |A002     |A002     |US     |\n",
      "|1013|Soumyajit  |Pal          |A002     |A002     |US     |\n",
      "|1018|Rahul      |Roychowdhury |A003     |A003     |UK     |\n",
      "|1016|Anamika    |Pal          |A002     |A002     |US     |\n",
      "|1017|Swaralipi  |Roy          |A002     |A002     |US     |\n",
      "|1019|Paulomi    |Mondal       |A004     |A004     |UAE    |\n",
      "|1021|Avirupa    |Ghosh        |A004     |A004     |UAE    |\n",
      "+----+-----------+-------------+---------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPersonWithAddress = dfPerson.join(dfAddress.hint(\"SHUFFLE_HASH\"), dfPerson.AddressId == dfAddress.AddressId, \"inner\")\n",
    "dfPersonWithAddress.printSchema()\n",
    "display(dfPersonWithAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e938ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- ShuffledHashJoin [AddressId#3], [AddressId#25], Inner, BuildRight\n",
      "   :- Exchange hashpartitioning(AddressId#3, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      "   :  +- Filter isnotnull(AddressId#3)\n",
      "   :     +- Scan ExistingRDD[Id#0L,First_Name#1,Last_Name#2,AddressId#3]\n",
      "   +- Exchange hashpartitioning(AddressId#25, 200), ENSURE_REQUIREMENTS, [plan_id=280]\n",
      "      +- Filter isnotnull(AddressId#25)\n",
      "         +- Scan ExistingRDD[AddressId#25,Address#26]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPersonWithAddress.explain()\n",
    "\n",
    "# In the “Physical Plan”, it can be seen that the “Join” performed is “Shuffle Hash Join” indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3c37f",
   "metadata": {},
   "source": [
    "#### 3. Broadcast Hash Join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d83ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Broadcast Hash Join by setting the threshold to -1\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10485760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2d50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10485760\n"
     ]
    }
   ],
   "source": [
    "# 10 MB = 10 * 1024 * 1024 Bytes = 10485760 Bytes\n",
    "print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506d62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- First_Name: string (nullable = true)\n",
      " |-- Last_Name: string (nullable = true)\n",
      " |-- AddressId: string (nullable = true)\n",
      " |-- AddressId: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      "\n",
      "+----+-----------+-------------+---------+---------+-------+\n",
      "|Id  |First_Name |Last_Name    |AddressId|AddressId|Address|\n",
      "+----+-----------+-------------+---------+---------+-------+\n",
      "|1001|Oindrila   |Chakraborty  |A001     |A001     |India  |\n",
      "|1002|Soumyajyoti|Bagchi       |A002     |A002     |US     |\n",
      "|1003|Oishi      |Bhattacharyya|A004     |A004     |UAE    |\n",
      "|1004|Sabarni    |Chakraborty  |A003     |A003     |UK     |\n",
      "|1005|Ayan       |Dutta        |A002     |A002     |US     |\n",
      "|1006|Dhrubajyoti|Das          |A004     |A004     |UAE    |\n",
      "|1007|Sayantan   |Chatterjee   |A004     |A004     |UAE    |\n",
      "|1008|Ritwik     |Ghosh        |A001     |A001     |India  |\n",
      "|1009|Puja       |Bhatt        |A001     |A001     |India  |\n",
      "|1010|Souvik     |Roy          |A002     |A002     |US     |\n",
      "|1011|Souvik     |Roy          |A003     |A003     |UK     |\n",
      "|1012|Ria        |Ghosh        |A003     |A003     |UK     |\n",
      "|1013|Soumyajit  |Pal          |A002     |A002     |US     |\n",
      "|1014|Abhirup    |Chakraborty  |A004     |A004     |UAE    |\n",
      "|1015|Sagarneel  |Sarkar       |A003     |A003     |UK     |\n",
      "|1016|Anamika    |Pal          |A002     |A002     |US     |\n",
      "|1017|Swaralipi  |Roy          |A002     |A002     |US     |\n",
      "|1018|Rahul      |Roychowdhury |A003     |A003     |UK     |\n",
      "|1019|Paulomi    |Mondal       |A004     |A004     |UAE    |\n",
      "|1020|Avishek    |Basu         |A002     |A002     |US     |\n",
      "+----+-----------+-------------+---------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "dfPersonWithAddress = dfPerson.join(broadcast(dfAddress), dfPerson.AddressId == dfAddress.AddressId, \"inner\")\n",
    "dfPersonWithAddress.printSchema()\n",
    "display(dfPersonWithAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f2a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [AddressId#3], [AddressId#25], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(AddressId#3)\n",
      "   :  +- Scan ExistingRDD[Id#0L,First_Name#1,Last_Name#2,AddressId#3]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=365]\n",
      "      +- Filter isnotnull(AddressId#25)\n",
      "         +- Scan ExistingRDD[AddressId#25,Address#26]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "dfPersonWithAddress.explain()\n",
    "\n",
    "# In the “Physical Plan”, it can be seen that the “Join” performed is “Broadcast Hash Join” indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044dd8b",
   "metadata": {},
   "source": [
    "#### 4. Iterative Broadcast Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb99f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First change the BrodcastJoinThreashold from 10MB to 100 MB\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "print(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the small table and persist in Memory\n",
    "df_small = spark.read.parquet(path)\n",
    "df_small_persist= df_small.persist()\n",
    "df_small_persist.count()\n",
    "\n",
    "#df_small.queryExecution.logical.optimizedPlan.stats.sizeInBytes\n",
    "\n",
    "# Step 2: Get the size of the small table. Assume result gives 400MB\n",
    "fst_txn_sz=df_small.queryExecution.logical.optimizedPlan.stats.sizeInBytes\n",
    "\n",
    "# Step 3: Convert the No Of Chunk that the smaller table will be divide by dividing the size by BroadCastThreadshold Value which will give 4 as result.\n",
    "no_of_chunk=(fst_txn_sz/(1024*1024*100)).toInt\n",
    "\n",
    "# Step 4: Now divide the Smallest table into no_of_chuck by randomly assign the number to each record out of no of chuck. To acheive that we can use monotonically_increasing_id() function as shown below\n",
    "df_small_broadcast=df_small.withColumn(\"chunk_id\",monotonically_increasing_id()%num_of_pass)\n",
    "\n",
    "# Step 5 : Iterate to Each broadcast and join with Largest table. Either you can union it or write it to tempory location then again join it. I prefer the write to temperory location.\n",
    "for chunk in range(no_of_chunk):\n",
    "  if(chunk%2==0):\n",
    "    df_largest=spark.read.parquet(temp_path_even)\n",
    "  else:\n",
    "    df_largest=spark.read.parquet(temp_path_odd)\n",
    "\n",
    "  df_larget.join(df_small_broadcast.where(chunk_id=chunk),column_condition,join)\n",
    "\n",
    "  if(chunk&2==0 and chunk<no_of_chunk):\n",
    "    spark.write.mode(\"overwrite\").parquet(temp_path_odd)\n",
    "  elif(chunk%2==1 and chunk<no_of_chunk):\n",
    "    spark.write.mode(\"overwrite\").parquet(temp_path_even)\n",
    "  else:\n",
    "    print(\"Done\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
