{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2912e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.ui.port\", \"4042\")\n",
    "    .set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    "    .set(\"spark.sql.warehouse.dir\", \"s3a://defaultfs/spark/warehouse\")\n",
    "    .set(\"spark.hadoop.fs.defaultFS\", \"s3a://defaultfs/\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "# create Spark context with Spark configuration\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark-hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(conf=sparkConf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark\n",
    "\n",
    "print(spark.sparkContext.appName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afeda1",
   "metadata": {},
   "source": [
    "#### Setup HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sc = SparkContext(conf=sparkConf)\n",
    "sqlContext = HiveContext(sc)\n",
    "\n",
    "#\n",
    "df_07 = sqlContext.sql(\"SELECT * from sample_07\")\n",
    "df_07.filter(df_07.salary > 150000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "transactionDF = spark.sql(\"SELECT * from transaction_details\")\n",
    "transactionDF.printSchema()\n",
    "transactionDF.filter(transactionDF.amount > 10000).show()\n",
    "\n",
    "#\n",
    "#df_08 = spark.sql(\"SELECT * from sample_08\")\n",
    "#tbls = spark.sql(\"show tables\")\n",
    "#tbls.show()\n",
    "\n",
    "#\n",
    "#df_09 = df_07.join(df_08, df_07.code == df_08.code).select(df_07.code, df_07.description)\n",
    "#df_09.show()\n",
    "\n",
    "#\n",
    "#df_09.write.saveAsTable(\"sample_09\")\n",
    "#tbls = spark.sql(\"show tables\")\n",
    "#tbls.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
