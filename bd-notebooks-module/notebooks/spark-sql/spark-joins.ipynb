{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c71f87",
   "metadata": {},
   "source": [
    "#### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e329a04",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.sql.warehouse.dir\", \"s3a://defaultfs/spark/warehouse\")\n",
    "    #.set(\"spark.hadoop.fs.defaultFS\", \"s3a://defaultfs/\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-joins-notebook').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33638a0c",
   "metadata": {},
   "source": [
    "### Spark Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d002b8f",
   "metadata": {},
   "source": [
    "#### Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc556031",
   "metadata": {},
   "source": [
    "#### Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9a9be",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employee_columns = ['emp_id', 'emp_name', 'emp_role', 'emp_manager', 'emp_hiredate', 'emp_salary', 'emp_comm', 'emp_dept']\n",
    "\n",
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    schema=employee_schema\n",
    ")\n",
    "\n",
    "#\n",
    "# employee_df.printSchema()\n",
    "\n",
    "#\n",
    "# print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dept_columns = ['dept_id', 'dept_name', 'dept_location']\n",
    "\n",
    "dept_schema = StructType() \\\n",
    "    .add(\"dept_id\", IntegerType(), True) \\\n",
    "    .add(\"dept_name\", StringType(), True) \\\n",
    "    .add(\"dept_location\", StringType(), True)\n",
    "\n",
    "dept_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(dept_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/departments.csv\")\n",
    "\n",
    "#dept_df.printSchema()\n",
    "\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98515a",
   "metadata": {},
   "source": [
    "### Spark Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6822fe",
   "metadata": {},
   "source": [
    "#### Inner Join\n",
    "Returns only the rows from both the dataframes that have matching values in both columns specified as the join keys.\n",
    "\n",
    "```sql\n",
    "df1.join(df2, df1['key'] == df2['key'], 'inner').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.join(dept_df, employee_df[\"emp_dept\"] == dept_df[\"dept_id\"], \"inner\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ed9f0",
   "metadata": {},
   "source": [
    "#### Left / Left Outer Join\n",
    "Returns all the rows from the left dataframe and the matching rows from the right dataframe. If there are no matching values in the right dataframe, then it returns a null.\n",
    "\n",
    "`Syntax`\n",
    "```sql\n",
    "df1.join(df2, df1['key'] == df2['key'], 'left').show()\n",
    "(OR)\n",
    "df1.join(df2, df1['key'] == df2['key'], 'leftouter').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.join(dept_df, employee_df[\"emp_dept\"] == dept_df[\"dept_id\"], \"leftouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c9903",
   "metadata": {},
   "source": [
    "#### Right / Right Outer Join\n",
    "```\n",
    "df1.join(df2, df1['key'] == df2['key'], 'right').show()\n",
    "(OR)\n",
    "df1.join(df2, df1['key'] == df2['key'], 'rightouter').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c42d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.join(dept_df, employee_df[\"emp_dept\"] == dept_df[\"dept_id\"], \"right\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58481e64",
   "metadata": {},
   "source": [
    "#### Outer / Full Join\n",
    "\n",
    "```sql\n",
    "df1.join(df2, df1['key'] == df2['key'], 'outer').show()\n",
    "(OR)\n",
    "df1.join(df2, df1['key'] == df2['key'], 'full').show()\n",
    "(OR)\n",
    "df1.join(df2, df1['key'] == df2['key'], 'fullouter').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.join(dept_df, employee_df[\"emp_dept\"] == dept_df[\"dept_id\"], \"outer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79f311",
   "metadata": {},
   "source": [
    "### Cross Join\n",
    "\n",
    "```sql\n",
    "df1.crossJoin(df2).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbebd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.crossJoin(dept_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec1073",
   "metadata": {},
   "source": [
    "#### Left Anti Join\n",
    "A left anti join in Spark SQL is a type of left join operation that returns only the rows from the left dataframe that do not have matching values in the right dataframe. It is used to find the rows in one dataframe that do not have corresponding values in another dataframe.\n",
    "\n",
    "```sql\n",
    "df1.join(df2, df1['key'] == df2['key'], 'left_anti').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.join(dept_df, employee_df[\"emp_dept\"] == dept_df[\"dept_id\"], \"left_anti\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04528d67",
   "metadata": {},
   "source": [
    "#### Left Semi Join\n",
    "A left semi join in Spark SQL is a type of join operation that returns only the columns from the left dataframe that have matching values in the right dataframe. It is used to find the values in one dataframe that have corresponding values in another dataframe.\n",
    "\n",
    "```sql\n",
    "df1.join(df2, df1['key'] == df2['key'], 'leftsemi').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.join(dept_df, employee_df[\"emp_dept\"] == dept_df[\"dept_id\"], \"leftsemi\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04343a7c",
   "metadata": {},
   "source": [
    "#### Self Join\n",
    "A self join in Spark SQL is a join operation in which a dataframe is joined with itself. It is used to compare the values within a single dataframe and return the rows that match specified criteria.\n",
    "\n",
    "```sql\n",
    "df.alias(\"df1\").join(df.alias(\"df2\"), df1['key'] == df2['key']).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18078631",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.alias(\"employee_df1\").join(employee_df.alias(\"employee_df2\"), employee_df['emp_id'] == employee_df['emp_id']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
