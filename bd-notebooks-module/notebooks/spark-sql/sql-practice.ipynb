{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c71f87",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e329a04",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/03 09:58:22 WARN Utils: Your hostname, vmware-ubuntu-24.04 resolves to a loopback address: 127.0.1.1; using 192.168.154.133 instead (on interface ens33)\n",
      "25/10/03 09:58:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/03 09:58:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.154.133:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-sql-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x78169bdd4250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.ui.port\", \"4042\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    #.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    #.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-sql-notebook').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33638a0c",
   "metadata": {},
   "source": [
    "### Spark Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d002b8f",
   "metadata": {},
   "source": [
    "#### Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0050eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc556031",
   "metadata": {},
   "source": [
    "#### Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "\n",
    "# Check Spark defaultParallelism\n",
    "print(spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859db39",
   "metadata": {},
   "source": [
    "### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0396804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\", \"TX\":\"Texas\", \"CH\":\"Chicago\"} \n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "print(\"{}\".format(broadcastStates.value))\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)\n",
    "\n",
    "## Broadcast variable on filter\n",
    "filteDf= df.where((df['state'].isin(list(broadcastStates.value.keys()))))\n",
    "filteDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea47e3e",
   "metadata": {},
   "source": [
    "### Accumulators Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf89ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Accumulator\n",
    "sc_acc = spark.sparkContext.accumulator(0)\n",
    "print(\"Accumulator initial value: {}\".format(sc_acc.value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9a9be",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## By default, Spark's CSV reader treats empty strings (\"\") and blank values (e.g., ,, where nothing is between the commas) as null\n",
    "\n",
    "# This code will read \"N/A\" as null\n",
    "dfFromCSV = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"delimiter\", ',') \\\n",
    "    .option(\"emptyValue\", 'N/A') \\\n",
    "    .option(\"nullValue\", 0) \\\n",
    "    .option(\"treatEmptyValuesAsNulls\", True) \\\n",
    "    .options(inferSchema=True, delimiter=',') \\\n",
    "    .csv(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "\n",
    "#dfFromCSV.printSchema()\n",
    "dfFromCSV.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01dd611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallCharge: long (nullable = true)\n",
      " |-- DateTime: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginatingNum: long (nullable = true)\n",
      " |-- TerminatingNum: long (nullable = true)\n",
      "\n",
      "+----------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|CallCharge|DateTime           |Dest          |Origin        |OriginatingNum|TerminatingNum|\n",
      "+----------+-------------------+--------------+--------------+--------------+--------------+\n",
      "|549       |02/11/2016 01:51:41|Birmingham    |London        |797308107     |797131221     |\n",
      "|2645      |05/02/2016 01:26:54|London        |Manchester    |777121117     |777440392     |\n",
      "|1233      |01/12/2016 21:12:54|Manchester    |Victoria      |797009202     |784243404     |\n",
      "|2651      |07/11/2016 01:07:34|Victoria      |Twickenham    |777557705     |798420467     |\n",
      "|3162      |02/11/2016 22:22:26|Scotland      |Leeds         |785434022     |779086250     |\n",
      "|2246      |05/01/2016 20:12:35|Virginia Water|Bradford      |779716202     |795137353     |\n",
      "|571       |04/12/2016 23:53:52|Ascot         |Yorkshire     |775490102     |775019605     |\n",
      "|3291      |06/11/2016 20:31:49|Bracknell     |Birmingham    |787581376     |797043387     |\n",
      "|2270      |03/12/2016 12:15:17|Bradford      |Coventary     |789231956     |787649491     |\n",
      "|3420      |06/02/2016 20:57:44|Yorkshire     |Wales         |785969980     |789993090     |\n",
      "|3084      |02/01/2016 02:44:27|Birmingham    |Scotland      |797662091     |777765510     |\n",
      "|3037      |09/01/2016 00:48:43|Marlow        |Virginia Water|784036802     |798095485     |\n",
      "|3011      |08/11/2016 20:19:19|Sunningdale   |Ascot         |785160169     |797922170     |\n",
      "|1018      |05/01/2016 11:24:28|Lords         |Bracknell     |789519210     |774080821     |\n",
      "|771       |02/12/2016 02:07:09|Oval          |Marlow        |775617249     |786549418     |\n",
      "|3585      |07/11/2016 03:43:23|Coventary     |Sunningdale   |797932062     |788292522     |\n",
      "|908       |06/01/2016 23:08:06|Wales         |Lords         |777561966     |788455450     |\n",
      "|95        |04/12/2016 24:17:54|Scotland      |Oval          |777508024     |789954417     |\n",
      "|2754      |03/11/2016 00:45:24|Birmingham    |Birmingham    |777087537     |778710691     |\n",
      "|1327      |03/01/2016 03:11:03|Coventary     |London        |774688108     |797626213     |\n",
      "+----------+-------------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromJSON = spark.read.format(\"json\").load(\"file:///apps/sandbox/defaultfs/cdrs.json\")\n",
    "dfFromJSON.printSchema()\n",
    "dfFromJSON.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1090dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|1       |2021-04-01 00:00:18 |2021-04-01 00:21:54  |1.0            |8.4          |1.0       |N                 |79          |116         |1           |25.5       |3.0  |0.5    |5.85      |0.0         |0.3                  |35.15       |2.5                 |0.0        |\n",
      "|1       |2021-04-01 00:42:37 |2021-04-01 00:46:23  |1.0            |0.9          |1.0       |N                 |75          |236         |2           |5.0        |3.0  |0.5    |0.0       |0.0         |0.3                  |8.8         |2.5                 |0.0        |\n",
      "|1       |2021-04-01 00:57:56 |2021-04-01 01:08:22  |1.0            |3.4          |1.0       |N                 |236         |168         |2           |11.5       |3.0  |0.5    |0.0       |0.0         |0.3                  |15.3        |2.5                 |0.0        |\n",
      "|1       |2021-04-01 00:01:58 |2021-04-01 00:54:27  |1.0            |0.0          |1.0       |N                 |47          |61          |1           |44.2       |0.0  |0.5    |0.0       |0.0         |0.3                  |45.0        |0.0                 |0.0        |\n",
      "|2       |2021-04-01 00:24:55 |2021-04-01 00:34:33  |1.0            |1.96         |1.0       |N                 |238         |152         |1           |9.0        |0.5  |0.5    |3.09      |0.0         |0.3                  |13.39       |0.0                 |0.0        |\n",
      "|2       |2021-04-01 00:19:16 |2021-04-01 00:21:46  |1.0            |0.77         |1.0       |N                 |142         |238         |1           |4.5        |0.5  |0.5    |1.24      |0.0         |0.3                  |9.54        |2.5                 |0.0        |\n",
      "|2       |2021-04-01 00:25:11 |2021-04-01 00:31:53  |1.0            |3.65         |1.0       |N                 |238         |244         |1           |11.5       |0.5  |0.5    |2.56      |0.0         |0.3                  |15.36       |0.0                 |0.0        |\n",
      "|1       |2021-04-01 00:27:53 |2021-04-01 00:47:03  |0.0            |8.9          |1.0       |N                 |138         |239         |1           |26.5       |3.0  |0.5    |7.25      |6.12        |0.3                  |43.67       |2.5                 |0.0        |\n",
      "|2       |2021-04-01 00:24:24 |2021-04-01 00:37:50  |1.0            |2.98         |1.0       |N                 |151         |244         |2           |12.0       |0.5  |0.5    |0.0       |0.0         |0.3                  |13.3        |0.0                 |0.0        |\n",
      "|1       |2021-04-01 00:19:18 |2021-04-01 00:41:25  |1.0            |8.9          |1.0       |N                 |132         |196         |2           |28.0       |0.5  |0.5    |0.0       |0.0         |0.3                  |29.3        |0.0                 |0.0        |\n",
      "|2       |2021-04-01 00:04:25 |2021-04-01 00:29:22  |1.0            |7.48         |1.0       |N                 |140         |36          |1           |23.5       |0.5  |0.5    |4.0       |0.0         |0.3                  |31.3        |2.5                 |0.0        |\n",
      "|2       |2021-04-01 00:03:07 |2021-04-01 00:18:02  |5.0            |3.39         |1.0       |N                 |231         |48          |2           |13.5       |0.5  |0.5    |0.0       |0.0         |0.3                  |17.3        |2.5                 |0.0        |\n",
      "|2       |2021-04-01 00:35:44 |2021-04-01 00:51:06  |5.0            |3.51         |1.0       |N                 |48          |41          |1           |14.0       |0.5  |0.5    |0.0       |0.0         |0.3                  |17.8        |2.5                 |0.0        |\n",
      "|2       |2021-04-01 00:52:32 |2021-04-01 01:04:41  |5.0            |3.42         |1.0       |N                 |41          |120         |1           |12.5       |0.5  |0.5    |2.76      |0.0         |0.3                  |16.56       |0.0                 |0.0        |\n",
      "|2       |2021-04-01 00:28:05 |2021-04-01 00:47:59  |3.0            |12.14        |1.0       |N                 |132         |160         |1           |33.5       |0.5  |0.5    |10.44     |0.0         |0.3                  |45.24       |0.0                 |0.0        |\n",
      "|1       |2021-04-01 00:39:01 |2021-04-01 00:57:39  |1.0            |11.8         |1.0       |N                 |132         |196         |2           |32.0       |0.5  |0.5    |0.0       |0.0         |0.3                  |33.3        |0.0                 |0.0        |\n",
      "|2       |2021-04-01 00:15:10 |2021-04-01 00:22:46  |1.0            |1.44         |1.0       |N                 |152         |244         |1           |7.5        |0.5  |0.5    |0.0       |0.0         |0.3                  |8.8         |0.0                 |0.0        |\n",
      "|2       |2021-04-01 00:30:46 |2021-04-01 00:39:52  |1.0            |1.65         |1.0       |N                 |161         |143         |1           |8.5        |0.5  |0.5    |3.08      |0.0         |0.3                  |15.38       |2.5                 |0.0        |\n",
      "|2       |2021-04-01 00:48:18 |2021-04-01 01:06:50  |2.0            |8.16         |1.0       |N                 |107         |106         |1           |25.0       |0.5  |0.5    |5.76      |0.0         |0.3                  |34.56       |2.5                 |0.0        |\n",
      "|1       |2021-04-01 00:19:42 |2021-04-01 00:33:25  |1.0            |7.4          |1.0       |N                 |138         |41          |1           |21.5       |0.5  |0.5    |6.0       |6.12        |0.3                  |34.92       |0.0                 |0.0        |\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfParquet = spark.read.format(\"parquet\").load(\"file:///apps/sandbox/defaultfs/taxi-data/yellow_tripdata_2021-04.parquet\")\n",
    "dfParquet.printSchema()\n",
    "dfParquet.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(employee_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "\n",
    "#df_with_schema.printSchema()\n",
    "\n",
    "df_with_schema.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67679c3a",
   "metadata": {},
   "source": [
    "### Handeling Null & Empyty Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Replacing null values with some value\n",
    "#\n",
    "df_with_schema.fillna(value=0).show()\n",
    "df_with_schema.fillna(value=0,subset=[\"emp_comm\"]).show() # only for emp_comm column\n",
    "df_with_schema.fillna(10,[\"emp_comm\"]) \\\n",
    "    .fillna(\"---\",[\"emp_manager\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Replacing null values with some value using na\n",
    "#\n",
    "df_with_schema.na.fill(value=0).show()\n",
    "df_with_schema.na.fill(value=0,subset=[\"emp_comm\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employee_columns = ['emp_id', 'emp_name', 'emp_role', 'emp_manager', 'emp_hiredate', 'emp_salary', 'emp_comm', 'emp_dept']\n",
    "\n",
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    schema=employee_schema\n",
    ")\n",
    "\n",
    "#\n",
    "# employee_df.printSchema()\n",
    "\n",
    "#\n",
    "# print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dept_columns = ['dept_id', 'dept_name', 'dept_location']\n",
    "\n",
    "dept_schema = StructType() \\\n",
    "    .add(\"dept_id\", IntegerType(), True) \\\n",
    "    .add(\"dept_name\", StringType(), True) \\\n",
    "    .add(\"dept_location\", StringType(), True)\n",
    "\n",
    "dept_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(dept_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/departments.csv\")\n",
    "\n",
    "#dept_df.printSchema()\n",
    "\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d11b66",
   "metadata": {},
   "source": [
    "### Data Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "employee_df = employee_df.repartition(2)\n",
    "\n",
    "#\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "# employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7a374",
   "metadata": {},
   "source": [
    "## Process Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82b525",
   "metadata": {},
   "source": [
    "### Map Dataframe Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50663e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def applyMap(row):\n",
    "    bonus = 0\n",
    "    salary = 0\n",
    "\n",
    "    if row.emp_comm is None :\n",
    "        bonus = 0\n",
    "    else :\n",
    "        bonus = row.emp_comm\n",
    "\n",
    "\n",
    "    if row.emp_role == 'ANALYST':\n",
    "        salary = row.emp_salary + 1000\n",
    "        bonus = bonus + 100\n",
    "    elif row.emp_role == 'CLERK':\n",
    "        salary = row.emp_salary + 1500\n",
    "        bonus = bonus + 150\n",
    "    elif row.emp_role == 'MANAGER':\n",
    "        salary = row.emp_salary + 2000\n",
    "        bonus = bonus + 200\n",
    "    elif row.emp_role == 'SALESMAN':\n",
    "        salary = row.emp_salary + 2500\n",
    "        bonus = bonus + 250\n",
    "    else:\n",
    "        salary = row.emp_salary\n",
    "        bonus = 0\n",
    "    \n",
    "    return (row.emp_id, row.emp_name, row.emp_role, row.emp_manager, row.emp_hiredate, salary, bonus, row.emp_dept)\n",
    "\n",
    "\n",
    "df1_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "\n",
    "##\n",
    "#df1 = employee_df.rdd.map(lambda r: (r.emp_id, r.emp_name, r.emp_role, r.emp_manager, r.emp_hiredate, r.emp_salary, 225, r.emp_dept)).toDF(df1_columns)\n",
    "\n",
    "##\n",
    "df1 = employee_df.rdd.map(applyMap).toDF(df1_columns)\n",
    "df1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cfaec",
   "metadata": {},
   "source": [
    "### flatMap Dataframe Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd64d6d",
   "metadata": {},
   "source": [
    "### Process Dataframe Partitions\n",
    "Unfortunately, PySpark DataFame doesnâ€™t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column. Below is a complete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a04123",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Spark is great\",), (\"Map and FlatMap are useful\",)]\n",
    "df = spark.createDataFrame(data, [\"sentence\"])\n",
    "\n",
    "# Use 'explode' function to achieve flatMap-like behavior\n",
    "df_words = df.withColumn(\"words\", explode(split(col(\"sentence\"), \" \")))\n",
    "\n",
    "df_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mapPartitions with yield\n",
    "def formatWithYield(partition_data):\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None :\n",
    "            bonus = 50\n",
    "        yield (record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df2 = employee_df.rdd.mapPartitions(formatWithYield).toDF(df2_columns)\n",
    "df2.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab38b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mapPartitions with iterator\n",
    "def formatWithIter(partition_data):\n",
    "    p_data = []\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        p_data.append([record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept])\n",
    "    return iter(p_data)\n",
    "\n",
    "df3_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df3 = employee_df.rdd.mapPartitions(formatWithIter).toDF(df3_columns)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38280f01",
   "metadata": {},
   "source": [
    "### Process Dataframe with mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "def formatWithMapPartitionsWithIndex(partitionIndex, iterator):\n",
    "    for paartion_data in iterator:\n",
    "        role = 'ANALYST' if  paartion_data.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = paartion_data.emp_salary\n",
    "        if paartion_data.emp_salary <= 1000 :\n",
    "            salary = paartion_data.emp_salary * 10\n",
    "        \n",
    "        bonus = paartion_data.emp_comm \n",
    "        if paartion_data.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        yield (partitionIndex+1, paartion_data.emp_id, paartion_data.emp_name, role, paartion_data.emp_manager, paartion_data.emp_hiredate, salary, bonus, paartion_data.emp_dept)\n",
    "    \n",
    "    #yield (partitionIndex, len(list(iterator)))\n",
    "    \n",
    "\n",
    "df4_columns = [\"partition\",\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df4 = employee_df.rdd.mapPartitionsWithIndex(formatWithMapPartitionsWithIndex).toDF(df4_columns)\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1,6)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select((df.id).alias(\"##\"), col(\"id\").alias(\"#ID\") ,(df.id + 10).alias('###COL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"id * 5\" , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = spark.createDataFrame(data=[(1000, 'Nick'), (1001, 'John'), (1002, 'Frank')], schema=['id', 'name'])\n",
    "names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ba19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3477967",
   "metadata": {},
   "source": [
    "#### Q-001. If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "`Adaptive Query Execution (AQE)`: In Spark 3.0 and later, Adaptive Query Execution (AQE) is an optimization that can dynamically adjust the number of shuffle partitions during runtime. AQE can coalesce small shuffle partitions into larger ones, effectively reducing the number of tasks if the data distribution allows for it. If AQE is enabled and determines that fewer partitions are optimal, you will see fewer than 200 tasks.\n",
    "\n",
    "`Data Volume and Distribution`: If the amount of data being processed is very small, or if the data is highly skewed (meaning a few keys have a disproportionately large amount of data), Spark might not utilize all 200 partitions efficiently, or AQE might optimize the partition count.\n",
    "\n",
    "`Coalesce or Repartition before GroupBy`: If a coalesce or repartition operation was performed on the DataFrame immediately before the groupBy, it could explicitly set the number of partitions, overriding the default shuffle partition setting for that specific operation.\n",
    "\n",
    "`Specific Optimization Strategies`: Certain optimization strategies or custom partitioning schemes might be in place that influence the number of partitions used during the groupBy operation, leading to a task count different from 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550893",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Sum of the first 100 whole numbers\n",
    "rdd = sc.parallelize(range(100 + 1))\n",
    "rdd.sum()\n",
    "# 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65902c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "## Load data from csv\n",
    "##\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "#employee_df.printSchema()\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()\n",
    "\n",
    "\n",
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036caa97",
   "metadata": {},
   "source": [
    "#### Get highest salary of each group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get highest salary of each group  \n",
    "w3 = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"row\",row_number().over(w3)) \\\n",
    "  .filter(col(\"row\") == 1).drop(\"row\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7936b75",
   "metadata": {},
   "source": [
    "#### Get max, min, avg, sum of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edab149",
   "metadata": {},
   "outputs": [],
   "source": [
    "w4 = Window.partitionBy(\"department\")\n",
    "df.withColumn(\"row\",row_number().over(w3)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(w4)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
