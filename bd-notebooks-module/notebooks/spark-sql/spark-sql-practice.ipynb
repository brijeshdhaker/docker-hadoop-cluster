{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c71f87",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e329a04",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/16 06:49:07 WARN Utils: Your hostname, vmware-ubuntu-24.04 resolves to a loopback address: 127.0.1.1; using 192.168.154.133 instead (on interface ens33)\n",
      "25/10/16 06:49:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/16 06:49:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.154.133:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-sql-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x772cd8bd4210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.ui.port\", \"4042\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    #.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    #.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    # Minio specific settings\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.sql.warehouse.dir\", \"s3a://defaultfs/spark/warehouse\")\n",
    "    #.set(\"spark.hadoop.fs.defaultFS\", \"s3a://defaultfs/\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-sql-notebook').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58886396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\", \"TX\":\"Texas\", \"CH\":\"Chicago\", \"OH\":\"Ohio\", \"NJ\":\"New Jersey\", \"UT\":\"Utah\" }\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "sample_data = [\n",
    "    ((\"James\", \"\", \"Smith\"), \"36636\", \"Sales\", 34, \"2006-01-01\", \"true\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'brown'}, \"M\", 3100.50, \"OH\", \"07-01-2019 12 01 19 406\"),\n",
    "    ((\"Michael\", \"Rose\", \"\"), \"40288\", \"Finance\", 25, \"2006-01-01\", \"true\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'brown', 'eye': 'brown'}, \"M\", 4300.65, \"CA\", \"06-24-2019 12 01 19 406\"),\n",
    "    ((\"Robert\", \"\", \"Williams\"), \"42114\", \"Marketing\", 27, \"1992-06-23\", \"false\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'black'}, \"M\", 1400.80, \"NJ\", \"11-16-2019 16 50 59 406\"),\n",
    "    ((\"Maria\", \"Anne\", \"Jones\"), \"39192\", \"Finance\", 19, \"1987-05-05\", \"false\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'brown'}, \"F\", 5500.75, \"NY\", \"11-16-2019 16 50 59 406\"),\n",
    "    ((\"Jen\", \"Mary\", \"Brown\"), \"39191\", \"Marketing\", 24, \"1990-01-01\", \"false\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'brown'}, \"F\", 0.00, \"UT\", \"11-16-2019 16 50 59 406\"),\n",
    "    ((\"Jeff\", \"A\", \"Jones\"), \"39193\", \"Finance\", 19, \"2000-07-17\", \"false\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'brown'}, \"F\", 5500.75, \"CA\", \"11-16-2019 16 50 59 406\"),\n",
    "    ((\"Robert\", \"\", \"Williams\"), \"42114\", \"Marketing\", 27, \"1992-06-23\", \"false\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'brown'}, \"M\", 1400.80, \"NJ\", \"11-16-2019 16 50 59 406\"),\n",
    "    ((\"James\", \"\", \"Smith\"), \"36636\", \"Sales\", 34, \"2006-01-01\", \"true\", [\"Java\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'brown'}, \"M\", 3100.50, \"OH\", \"07-01-2019 12 01 19 406\"),\n",
    "    ((\"Scott\", \"M\", \"Brown\"), \"39194\", \"Marketing\", 24, \"1994-10-01\", \"false\", [\"Python\", \"Scala\", \"C++\"], {'hair': 'black', 'eye': 'brown'}, \"F\", 0.00, None, \"11-16-2019 16 50 59 406\")\n",
    "]\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "sample_schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('department', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('join_date', StringType(), True),\n",
    "    StructField('isactive', StringType(), True),\n",
    "    StructField('languages', ArrayType(StringType(), True), True),\n",
    "    StructField('properties', MapType(StringType(),StringType()), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', FloatType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('timestamp', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62888f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- join_date: string (nullable = true)\n",
      " |-- isactive: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+---+----------+--------+--------------------+-----------------------------+------+-------+-----+-----------------------+\n",
      "|name                |id   |department|age|join_date |isactive|languages           |properties                   |gender|salary |state|timestamp              |\n",
      "+--------------------+-----+----------+---+----------+--------+--------------------+-----------------------------+------+-------+-----+-----------------------+\n",
      "|{James, , Smith}    |36636|Sales     |34 |2006-01-01|true    |[Java, Scala, C++]  |{eye -> brown, hair -> black}|M     |3100.5 |OH   |07-01-2019 12 01 19 406|\n",
      "|{Michael, Rose, }   |40288|Finance   |25 |2006-01-01|true    |[Java, Scala, C++]  |{eye -> brown, hair -> brown}|M     |4300.65|CA   |06-24-2019 12 01 19 406|\n",
      "|{Robert, , Williams}|42114|Marketing |27 |1992-06-23|false   |[Java, Scala, C++]  |{eye -> black, hair -> black}|M     |1400.8 |NJ   |11-16-2019 16 50 59 406|\n",
      "|{Maria, Anne, Jones}|39192|Finance   |19 |1987-05-05|false   |[Java, Scala, C++]  |{eye -> brown, hair -> black}|F     |5500.75|NY   |11-16-2019 16 50 59 406|\n",
      "|{Jen, Mary, Brown}  |39191|Marketing |24 |1990-01-01|false   |[Java, Scala, C++]  |{eye -> brown, hair -> black}|F     |0.0    |UT   |11-16-2019 16 50 59 406|\n",
      "|{Jeff, A, Jones}    |39193|Finance   |19 |2000-07-17|false   |[Java, Scala, C++]  |{eye -> brown, hair -> black}|F     |5500.75|CA   |11-16-2019 16 50 59 406|\n",
      "|{Robert, , Williams}|42114|Marketing |27 |1992-06-23|false   |[Java, Scala, C++]  |{eye -> brown, hair -> black}|M     |1400.8 |NJ   |11-16-2019 16 50 59 406|\n",
      "|{James, , Smith}    |36636|Sales     |34 |2006-01-01|true    |[Java, Scala, C++]  |{eye -> brown, hair -> black}|M     |3100.5 |OH   |07-01-2019 12 01 19 406|\n",
      "|{Scott, M, Brown}   |39194|Marketing |24 |1994-10-01|false   |[Python, Scala, C++]|{eye -> brown, hair -> black}|F     |0.0    |NULL |11-16-2019 16 50 59 406|\n",
      "+--------------------+-----+----------+---+----------+--------+--------------------+-----------------------------+------+-------+-----+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df = spark.createDataFrame(data = sample_data, schema = sample_schema)\n",
    "\n",
    "#\n",
    "df.printSchema()\n",
    "\n",
    "#\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e088000",
   "metadata": {},
   "source": [
    "#### Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097de5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|   id|department|\n",
      "+-----+----------+\n",
      "|42114| Marketing|\n",
      "|42114| Marketing|\n",
      "|40288|   Finance|\n",
      "|39194| Marketing|\n",
      "|39193|   Finance|\n",
      "|39192|   Finance|\n",
      "|39191| Marketing|\n",
      "|36636|     Sales|\n",
      "|36636|     Sales|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"department\").sort(desc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42061a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|   id|department|\n",
      "+-----+----------+\n",
      "|36636|     Sales|\n",
      "|36636|     Sales|\n",
      "|39191| Marketing|\n",
      "|39192|   Finance|\n",
      "|39193|   Finance|\n",
      "|39194| Marketing|\n",
      "|40288|   Finance|\n",
      "|42114| Marketing|\n",
      "|42114| Marketing|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"department\").sort(asc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff7169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|   id|department|\n",
      "+-----+----------+\n",
      "|36636|     Sales|\n",
      "|40288|   Finance|\n",
      "|42114| Marketing|\n",
      "|39192|   Finance|\n",
      "|39191| Marketing|\n",
      "|39193|   Finance|\n",
      "|39194| Marketing|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"department\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222568d",
   "metadata": {},
   "source": [
    "#### distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2f78e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|department|\n",
      "+----------+\n",
      "|     Sales|\n",
      "|   Finance|\n",
      "| Marketing|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"department\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fcc7c",
   "metadata": {},
   "source": [
    "#### Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "\n",
    "# Convert string to date\n",
    "df.select(\"name.*\", to_date(\"join_date\", \"yyyy-MM-dd\").alias(\"start_date\")).show(truncate=False)\n",
    "\n",
    "# Cast string to date\n",
    "df.withColumn(\"date_col\", col(\"join_date\").cast(\"date\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03706b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to date\n",
    "df.withColumn(\"date_col\", \n",
    "from_unixtime(unix_timestamp(\"join_date\", \"yyyy-MM-dd\")).cast(\"date\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "# Convert string to date\n",
    "df = df.withColumn(\"formatted_date\", \n",
    "date_format(\"join_date\", \"yyyy-MM-dd\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7771e9",
   "metadata": {},
   "source": [
    "#### Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaa4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'timestamp' column to a timestamp type\n",
    "# Specify the format of the input string\n",
    "df_converted = df.withColumn(\"add_ts\", to_timestamp(col(\"timestamp\"), \"MM-dd-yyyy HH mm ss SSS\"))\n",
    "\n",
    "# Show the updated DataFrame and its schema to verify the data type\n",
    "df_converted.show(truncate=False)\n",
    "\n",
    "# Cast string to timestamp\n",
    "df.withColumn(\"timestamp_col\", col(\"timestamp\").cast(\"timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17431db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "# Convert string to timestamp\n",
    "df.withColumn(\"timestamp_col\", \n",
    "from_unixtime(unix_timestamp(\"timestamp\", \"MM-dd-yyyy HH mm ss SSS\"))).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d4fc8",
   "metadata": {},
   "source": [
    "#### Select Nested columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns from struct type\n",
    "df.select(\"name.firstname\",\"name.middlename\",\"name.lastname\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f721336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all columns from struct type\n",
    "df.select(\"name.*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26581a",
   "metadata": {},
   "source": [
    "#### Explode Arry Type columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c252b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df.select(\"languages\", explode(col(\"languages\")).alias(\"speak_language\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df.withColumn(\"known_language\", explode(col(\"languages\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd723ef0",
   "metadata": {},
   "source": [
    "#### Access PySpark MapType Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3609d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df3=df.rdd.map(lambda x: (x.name, x.properties[\"hair\"], x.properties[\"eye\"])).toDF([\"name\",\"hair\",\"eye\"])\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n",
    "  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()\n",
    "\n",
    "df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n",
    "  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dee8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df.select(df.name,explode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdf3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_keys() – Get All Map Keys\n",
    "df.select(df.name,map_keys(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bded32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_values() – Get All map Values\n",
    "from pyspark.sql.functions import map_values\n",
    "df.select(df.name,map_values(df.properties)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33638a0c",
   "metadata": {},
   "source": [
    "### Spark Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d002b8f",
   "metadata": {},
   "source": [
    "#### Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0050eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc556031",
   "metadata": {},
   "source": [
    "#### Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "\n",
    "# Check Spark defaultParallelism\n",
    "print(spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859db39",
   "metadata": {},
   "source": [
    "### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0396804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "# print(\"{}\".format(broadcastStates.value))\n",
    "\n",
    "#\n",
    "def state_convert(code):\n",
    "    state_name = \"\"\n",
    "    if code is not None and code != \"\" : \n",
    "      state_name = broadcastStates.value[code] \n",
    "    return state_name\n",
    "\n",
    "#\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[10]))).toDF([\"name\",\"id\",\"department\",\"state_fullname\"])\n",
    "result.show(truncate=False)\n",
    "\n",
    "## Broadcast variable on filter\n",
    "filteDf= df.where((df['state'].isin(list(broadcastStates.value.keys()))))\n",
    "filteDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea47e3e",
   "metadata": {},
   "source": [
    "### Accumulators Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf89ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Accumulator\n",
    "sc_acc = spark.sparkContext.accumulator(0)\n",
    "print(\"Accumulator initial value: {}\".format(sc_acc.value))\n",
    "\n",
    "#\n",
    "data = range(1, 101)\n",
    "rdd_1 = spark.sparkContext.parallelize(data, 8)\n",
    "print(\"RDD-1 Partition Count : %i \" % (rdd_1.getNumPartitions()))\n",
    "print(\"Values in RDD-1 : {0} \".format(rdd_1.collect()))\n",
    "\n",
    "#\n",
    "# commons.print_separator()\n",
    "\n",
    "# Accessed By Driver\n",
    "rdd_1.foreach(lambda x: sc_acc.add(x))\n",
    "print(\"Total Sum  {0} \".format(sc_acc.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9a9be",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## By default, Spark's CSV reader treats empty strings (\"\") and blank values (e.g., ,, where nothing is between the commas) as null\n",
    "\n",
    "# This code will read \"N/A\" as null\n",
    "dfFromCSV = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"delimiter\", ',') \\\n",
    "    .option(\"emptyValue\", 'N/A') \\\n",
    "    .option(\"nullValue\", 0) \\\n",
    "    .option(\"treatEmptyValuesAsNulls\", True) \\\n",
    "    .options(inferSchema=True, delimiter=',') \\\n",
    "    .csv(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "\n",
    "#dfFromCSV.printSchema()\n",
    "dfFromCSV.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromJSON = spark.read.format(\"json\").load(\"file:///apps/sandbox/defaultfs/cdrs.json\")\n",
    "dfFromJSON.printSchema()\n",
    "dfFromJSON.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfParquet = spark.read.format(\"parquet\").load(\"file:///apps/sandbox/defaultfs/taxi-data/yellow_tripdata_2021-04.parquet\")\n",
    "dfParquet.printSchema()\n",
    "dfParquet.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(employee_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "\n",
    "#df_with_schema.printSchema()\n",
    "\n",
    "df_with_schema.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67679c3a",
   "metadata": {},
   "source": [
    "### Handeling Null & Empyty Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Replacing null values with some value\n",
    "#\n",
    "df_with_schema.fillna(value=0).show()\n",
    "df_with_schema.fillna(value=0,subset=[\"emp_comm\"]).show() # only for emp_comm column\n",
    "df_with_schema.fillna(10,[\"emp_comm\"]) \\\n",
    "    .fillna(\"---\",[\"emp_manager\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Replacing null values with some value using na\n",
    "#\n",
    "df_with_schema.na.fill(value=0).show()\n",
    "df_with_schema.na.fill(value=0,subset=[\"emp_comm\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eae2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "employee_columns = ['emp_id', 'emp_name', 'emp_role', 'emp_manager', 'emp_hiredate', 'emp_salary', 'emp_comm', 'emp_dept']\n",
    "\n",
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    schema=employee_schema\n",
    ")\n",
    "\n",
    "#\n",
    "# employee_df.printSchema()\n",
    "\n",
    "#\n",
    "# print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dept_columns = ['dept_id', 'dept_name', 'dept_location']\n",
    "\n",
    "dept_schema = StructType() \\\n",
    "    .add(\"dept_id\", IntegerType(), True) \\\n",
    "    .add(\"dept_name\", StringType(), True) \\\n",
    "    .add(\"dept_location\", StringType(), True)\n",
    "\n",
    "dept_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(dept_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/departments.csv\")\n",
    "\n",
    "#dept_df.printSchema()\n",
    "\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d11b66",
   "metadata": {},
   "source": [
    "### Data Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "employee_df = employee_df.repartition(2)\n",
    "\n",
    "#\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "# employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7a374",
   "metadata": {},
   "source": [
    "## Process Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82b525",
   "metadata": {},
   "source": [
    "### Map Dataframe Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50663e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def applyMap(row):\n",
    "    bonus = 0\n",
    "    salary = 0\n",
    "\n",
    "    if row.emp_comm is None :\n",
    "        bonus = 0\n",
    "    else :\n",
    "        bonus = row.emp_comm\n",
    "\n",
    "\n",
    "    if row.emp_role == 'ANALYST':\n",
    "        salary = row.emp_salary + 1000\n",
    "        bonus = bonus + 100\n",
    "    elif row.emp_role == 'CLERK':\n",
    "        salary = row.emp_salary + 1500\n",
    "        bonus = bonus + 150\n",
    "    elif row.emp_role == 'MANAGER':\n",
    "        salary = row.emp_salary + 2000\n",
    "        bonus = bonus + 200\n",
    "    elif row.emp_role == 'SALESMAN':\n",
    "        salary = row.emp_salary + 2500\n",
    "        bonus = bonus + 250\n",
    "    else:\n",
    "        salary = row.emp_salary\n",
    "        bonus = 0\n",
    "    \n",
    "    return (row.emp_id, row.emp_name, row.emp_role, row.emp_manager, row.emp_hiredate, salary, bonus, row.emp_dept)\n",
    "\n",
    "\n",
    "df1_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "\n",
    "##\n",
    "#df1 = employee_df.rdd.map(lambda r: (r.emp_id, r.emp_name, r.emp_role, r.emp_manager, r.emp_hiredate, r.emp_salary, 225, r.emp_dept)).toDF(df1_columns)\n",
    "\n",
    "##\n",
    "df1 = employee_df.rdd.map(applyMap).toDF(df1_columns)\n",
    "df1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cfaec",
   "metadata": {},
   "source": [
    "### flatMap Dataframe Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd64d6d",
   "metadata": {},
   "source": [
    "### Process Dataframe Partitions\n",
    "Unfortunately, PySpark DataFame doesn’t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column. Below is a complete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a04123",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "data = [(\"Spark is great\",), (\"Map and FlatMap are useful\",)]\n",
    "df = spark.createDataFrame(data, [\"sentence\"])\n",
    "\n",
    "# Use 'explode' function to achieve flatMap-like behavior\n",
    "df_words = df.withColumn(\"words\", explode(split(col(\"sentence\"), \" \")))\n",
    "\n",
    "df_words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mapPartitions with yield\n",
    "def formatWithYield(partition_data):\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None :\n",
    "            bonus = 50\n",
    "        yield (record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df2 = employee_df.rdd.mapPartitions(formatWithYield).toDF(df2_columns)\n",
    "df2.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab38b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mapPartitions with iterator\n",
    "def formatWithIter(partition_data):\n",
    "    p_data = []\n",
    "    for record in partition_data:\n",
    "        role = 'ANALYST' if  record.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = record.emp_salary\n",
    "        if record.emp_salary <= 1000 :\n",
    "            salary = record.emp_salary * 10\n",
    "        \n",
    "        bonus = record.emp_comm \n",
    "        if record.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        p_data.append([record.emp_id, record.emp_name, record.emp_role, record.emp_manager, record.emp_hiredate, salary, bonus, record.emp_dept])\n",
    "    return iter(p_data)\n",
    "\n",
    "df3_columns = [\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df3 = employee_df.rdd.mapPartitions(formatWithIter).toDF(df3_columns)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38280f01",
   "metadata": {},
   "source": [
    "### Process Dataframe with mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "def formatWithMapPartitionsWithIndex(partitionIndex, iterator):\n",
    "    for paartion_data in iterator:\n",
    "        role = 'ANALYST' if  paartion_data.emp_role == 'CLERK' else 'MANAGER'\n",
    "        salary = paartion_data.emp_salary\n",
    "        if paartion_data.emp_salary <= 1000 :\n",
    "            salary = paartion_data.emp_salary * 10\n",
    "        \n",
    "        bonus = paartion_data.emp_comm \n",
    "        if paartion_data.emp_comm is None:\n",
    "            bonus = 50\n",
    "        \n",
    "        yield (partitionIndex+1, paartion_data.emp_id, paartion_data.emp_name, role, paartion_data.emp_manager, paartion_data.emp_hiredate, salary, bonus, paartion_data.emp_dept)\n",
    "    \n",
    "    #yield (partitionIndex, len(list(iterator)))\n",
    "    \n",
    "\n",
    "df4_columns = [\"partition\",\"emp_id\",\"emp_name\", \"emp_role\", \"emp_manager\", \"emp_hiredate\", \"emp_salary\", \"emp_comm\", \"emp_dept\"]\n",
    "df4 = employee_df.rdd.mapPartitionsWithIndex(formatWithMapPartitionsWithIndex).toDF(df4_columns)\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1,6)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select((df.id).alias(\"##\"), col(\"id\").alias(\"#ID\") ,(df.id + 10).alias('###COL')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"id * 5\" , \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = spark.createDataFrame(data=[(1000, 'Nick'), (1001, 'John'), (1002, 'Frank')], schema=['id', 'name'])\n",
    "names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ba19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select('id', 'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3477967",
   "metadata": {},
   "source": [
    "#### Q-001. If this value is set to a number other than 200, the number of tasks in the shuffle stage will reflect that new value.\n",
    "\n",
    "`Adaptive Query Execution (AQE)`: In Spark 3.0 and later, Adaptive Query Execution (AQE) is an optimization that can dynamically adjust the number of shuffle partitions during runtime. AQE can coalesce small shuffle partitions into larger ones, effectively reducing the number of tasks if the data distribution allows for it. If AQE is enabled and determines that fewer partitions are optimal, you will see fewer than 200 tasks.\n",
    "\n",
    "`Data Volume and Distribution`: If the amount of data being processed is very small, or if the data is highly skewed (meaning a few keys have a disproportionately large amount of data), Spark might not utilize all 200 partitions efficiently, or AQE might optimize the partition count.\n",
    "\n",
    "`Coalesce or Repartition before GroupBy`: If a coalesce or repartition operation was performed on the DataFrame immediately before the groupBy, it could explicitly set the number of partitions, overriding the default shuffle partition setting for that specific operation.\n",
    "\n",
    "`Specific Optimization Strategies`: Certain optimization strategies or custom partitioning schemes might be in place that influence the number of partitions used during the groupBy operation, leading to a task count different from 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550893",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.select(col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Sum of the first 100 whole numbers\n",
    "rdd = sc.parallelize(range(100 + 1))\n",
    "rdd.sum()\n",
    "# 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65902c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///apps/sandbox/defaultfs/employee.csv\")\n",
    "## Load data from csv\n",
    "##\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    nullValue=\"NA\",\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "#employee_df.printSchema()\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.repartition(2)\n",
    "print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "employee_df = employee_df.filter(col(\"emp_salary\") > 2000)\\\n",
    "    .select(\"emp_id\", \"emp_name\", \"emp_dept\", \"emp_salary\")\\\n",
    "    .groupby(\"emp_dept\")\\\n",
    "    .count()\n",
    "\n",
    "\n",
    "employee_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036caa97",
   "metadata": {},
   "source": [
    "#### Get highest salary of each group  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get highest salary of each group  \n",
    "w3 = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"row\",row_number().over(w3)) \\\n",
    "  .filter(col(\"row\") == 1).drop(\"row\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7936b75",
   "metadata": {},
   "source": [
    "#### Get max, min, avg, sum of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edab149",
   "metadata": {},
   "outputs": [],
   "source": [
    "w4 = Window.partitionBy(\"department\")\n",
    "df.withColumn(\"row\",row_number().over(w3)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(w4)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        Row(name=\"James,,Smith\", lang=[\"Java\", \"Scala\", \"C++\"], state=\"CA\"),\n",
    "        Row(name=\"Michael,Rose,\", lang=[\"Spark\", \"Java\", \"C++\"], state=\"NJ\"),\n",
    "        Row(name=\"Robert,,Williams\", lang=[\"CSharp\", \"VB\"], state=\"NV\")\n",
    "]\n",
    "\n",
    "#DataFrame Example 1\n",
    "columns = [\"name\", \"languagesAtSchool\", \"currentState\"]\n",
    "df1 = spark.createDataFrame(data)\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "\n",
    "#DataFrame Example 2\n",
    "\n",
    "columns = [\"name\", \"languagesAtSchool\", \"currentState\"]\n",
    "df2 = spark.createDataFrame(data).toDF(*columns)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "structureData = [\n",
    "    ((\"James\", \"\", \"Smith\"), \"36636\", \"M\", 3100),\n",
    "    ((\"Michael\", \"Rose\", \"\"), \"40288\", \"M\", 4300),\n",
    "    ((\"Robert\", \"\", \"Williams\"), \"42114\", \"M\", 1400),\n",
    "    ((\"Brijesh\", \"K\", \"Dhaker\"), \"400706\", \"M\", 3000),\n",
    "    ((\"Maria\", \"Anne\", \"Jones\"), \"39192\", \"F\", 5500),\n",
    "    ((\"Jen\", \"Mary\", \"Brown\"), \"\", \"F\", -1)\n",
    "]\n",
    "structureSchema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data=structureData, schema=structureSchema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "#\n",
    "#\n",
    "#\n",
    "updatedDF = df2.withColumn(\"OtherInfo\",\n",
    "                           struct(col(\"id\").alias(\"identifier\"),\n",
    "                                  col(\"gender\").alias(\"gender\"),\n",
    "                                  col(\"salary\").alias(\"salary\"),\n",
    "                                  when(col(\"salary\").cast(IntegerType()) < 2000, \"Low\")\n",
    "                                  .when(col(\"salary\").cast(IntegerType()) < 4000, \"Medium\")\n",
    "                                  .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "                           )).drop(\"id\", \"gender\", \"salary\")\n",
    "\n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
