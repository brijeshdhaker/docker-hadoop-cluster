{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36b94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/13 08:50:38 WARN Utils: Your hostname, vmware-ubuntu-24.04 resolves to a loopback address: 127.0.1.1; using 192.168.154.133 instead (on interface ens33)\n",
      "25/10/13 08:50:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/13 08:50:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.154.133:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-joins-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7bca8f84cd10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    #.set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0\")\n",
    "    #.set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    #.set(\"spark.network.timeout\", \"400000\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    #.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.sql.warehouse.dir\", \"s3a://defaultfs/spark/warehouse\")\n",
    "    #.set(\"spark.hadoop.fs.defaultFS\", \"s3a://defaultfs/\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-hints').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275d29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|emp_id|emp_name|emp_role |emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "|7839  |KING    |PRESIDENT|NULL       |1981-11-17  |5000      |NULL    |10      |\n",
      "|7698  |BLAKE   |MANAGER  |7839       |1981-01-05  |2850      |125     |30      |\n",
      "|7782  |CLARK   |MANAGER  |7839       |1983-09-06  |2450      |NULL    |10      |\n",
      "|7566  |JONES   |MANAGER  |7839       |1981-02-04  |2975      |345     |20      |\n",
      "|7788  |SCOTT   |ANALYST  |7566       |1984-12-17  |3000      |NULL    |20      |\n",
      "|7902  |FORD    |ANALYST  |7566       |1981-11-27  |3000      |NULL    |40      |\n",
      "|7369  |SMITH   |CLERK    |7902       |1980-04-04  |800       |NULL    |20      |\n",
      "|7499  |ALLEN   |SALESMAN |7698       |1981-05-05  |1600      |300     |30      |\n",
      "|7521  |WARD    |SALESMAN |7698       |1982-06-23  |1250      |500     |50      |\n",
      "|7654  |MARTIN  |SALESMAN |7698       |1981-07-21  |1250      |1400    |30      |\n",
      "|7844  |TURNER  |SALESMAN |7698       |1982-09-08  |1500      |0       |40      |\n",
      "|7876  |ADAMS   |CLERK    |7788       |1980-11-14  |1100      |100     |20      |\n",
      "|7900  |JAMES   |CLERK    |7788       |1983-08-05  |950       |NULL    |60      |\n",
      "|7934  |MILLER  |CLERK    |7788       |1982-01-02  |1300      |225     |50      |\n",
      "|7935  |ROBERT  |ANALYST  |7566       |1984-03-31  |1940      |225     |10      |\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_columns = ['emp_id', 'emp_name', 'emp_role', 'emp_manager', 'emp_hiredate', 'emp_salary', 'emp_comm', 'emp_dept']\n",
    "\n",
    "employee_schema = StructType() \\\n",
    "    .add(\"emp_id\", IntegerType(), True) \\\n",
    "    .add(\"emp_name\", StringType(), True) \\\n",
    "    .add(\"emp_role\", StringType(), True) \\\n",
    "    .add(\"emp_manager\", StringType(), True) \\\n",
    "    .add(\"emp_hiredate\", DateType(), True) \\\n",
    "    .add(\"emp_salary\", IntegerType(), True) \\\n",
    "    .add(\"emp_comm\", IntegerType(), True) \\\n",
    "    .add(\"emp_dept\", IntegerType(), True)\n",
    "\n",
    "employee_df = spark.read.csv(\"file:///apps/sandbox/defaultfs/employee.csv\",\n",
    "    header=True,\n",
    "    schema=employee_schema\n",
    ")\n",
    "\n",
    "#\n",
    "# employee_df.printSchema()\n",
    "\n",
    "#\n",
    "# print(employee_df.rdd.getNumPartitions())\n",
    "\n",
    "#\n",
    "employee_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7b5368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+\n",
      "|dept_id|dept_name |dept_location|\n",
      "+-------+----------+-------------+\n",
      "|10     |ACCOUNTING|NY           |\n",
      "|20     |RESEARCH  |TX           |\n",
      "|30     |SALES     |CH           |\n",
      "|40     |OPERATIONS|TX           |\n",
      "|50     |ADMIN     |CA           |\n",
      "|70     |SECURITY  |FL           |\n",
      "+-------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_columns = ['dept_id', 'dept_name', 'dept_location']\n",
    "\n",
    "dept_schema = StructType() \\\n",
    "    .add(\"dept_id\", IntegerType(), True) \\\n",
    "    .add(\"dept_name\", StringType(), True) \\\n",
    "    .add(\"dept_location\", StringType(), True)\n",
    "\n",
    "dept_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(dept_schema) \\\n",
    "    .load(\"file:///apps/sandbox/defaultfs/departments.csv\")\n",
    "\n",
    "#dept_df.printSchema()\n",
    "\n",
    "dept_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c6fde",
   "metadata": {},
   "source": [
    "#### 1. Join Strategy Hints: \n",
    "These hints influence how Spark performs joins between DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66552c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|emp_id|emp_name| emp_role|emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|dept_id| dept_name|dept_location|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|  7839|    KING|PRESIDENT|       NULL|  1981-11-17|      5000|    NULL|      10|     10|ACCOUNTING|           NY|\n",
      "|  7698|   BLAKE|  MANAGER|       7839|  1981-01-05|      2850|     125|      30|     30|     SALES|           CH|\n",
      "|  7782|   CLARK|  MANAGER|       7839|  1983-09-06|      2450|    NULL|      10|     10|ACCOUNTING|           NY|\n",
      "|  7566|   JONES|  MANAGER|       7839|  1981-02-04|      2975|     345|      20|     20|  RESEARCH|           TX|\n",
      "|  7788|   SCOTT|  ANALYST|       7566|  1984-12-17|      3000|    NULL|      20|     20|  RESEARCH|           TX|\n",
      "|  7902|    FORD|  ANALYST|       7566|  1981-11-27|      3000|    NULL|      40|     40|OPERATIONS|           TX|\n",
      "|  7369|   SMITH|    CLERK|       7902|  1980-04-04|       800|    NULL|      20|     20|  RESEARCH|           TX|\n",
      "|  7499|   ALLEN| SALESMAN|       7698|  1981-05-05|      1600|     300|      30|     30|     SALES|           CH|\n",
      "|  7521|    WARD| SALESMAN|       7698|  1982-06-23|      1250|     500|      50|     50|     ADMIN|           CA|\n",
      "|  7654|  MARTIN| SALESMAN|       7698|  1981-07-21|      1250|    1400|      30|     30|     SALES|           CH|\n",
      "|  7844|  TURNER| SALESMAN|       7698|  1982-09-08|      1500|       0|      40|     40|OPERATIONS|           TX|\n",
      "|  7876|   ADAMS|    CLERK|       7788|  1980-11-14|      1100|     100|      20|     20|  RESEARCH|           TX|\n",
      "|  7934|  MILLER|    CLERK|       7788|  1982-01-02|      1300|     225|      50|     50|     ADMIN|           CA|\n",
      "|  7935|  ROBERT|  ANALYST|       7566|  1984-03-31|      1940|     225|      10|     10|ACCOUNTING|           NY|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# BROADCAST: Suggests that Spark broadcast a smaller DataFrame to all worker nodes when performing a join, which can be beneficial for small DataFrames to avoid shuffling.\n",
    "#\n",
    "employee_df.join(dept_df.hint(\"broadcast\"), employee_df[\"emp_dept\"] == dept_df[\"dept_id\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|emp_id|emp_name| emp_role|emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|dept_id| dept_name|dept_location|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|  7839|    KING|PRESIDENT|       NULL|  1981-11-17|      5000|    NULL|      10|     10|ACCOUNTING|           NY|\n",
      "|  7782|   CLARK|  MANAGER|       7839|  1983-09-06|      2450|    NULL|      10|     10|ACCOUNTING|           NY|\n",
      "|  7935|  ROBERT|  ANALYST|       7566|  1984-03-31|      1940|     225|      10|     10|ACCOUNTING|           NY|\n",
      "|  7566|   JONES|  MANAGER|       7839|  1981-02-04|      2975|     345|      20|     20|  RESEARCH|           TX|\n",
      "|  7788|   SCOTT|  ANALYST|       7566|  1984-12-17|      3000|    NULL|      20|     20|  RESEARCH|           TX|\n",
      "|  7369|   SMITH|    CLERK|       7902|  1980-04-04|       800|    NULL|      20|     20|  RESEARCH|           TX|\n",
      "|  7876|   ADAMS|    CLERK|       7788|  1980-11-14|      1100|     100|      20|     20|  RESEARCH|           TX|\n",
      "|  7698|   BLAKE|  MANAGER|       7839|  1981-01-05|      2850|     125|      30|     30|     SALES|           CH|\n",
      "|  7499|   ALLEN| SALESMAN|       7698|  1981-05-05|      1600|     300|      30|     30|     SALES|           CH|\n",
      "|  7654|  MARTIN| SALESMAN|       7698|  1981-07-21|      1250|    1400|      30|     30|     SALES|           CH|\n",
      "|  7902|    FORD|  ANALYST|       7566|  1981-11-27|      3000|    NULL|      40|     40|OPERATIONS|           TX|\n",
      "|  7844|  TURNER| SALESMAN|       7698|  1982-09-08|      1500|       0|      40|     40|OPERATIONS|           TX|\n",
      "|  7521|    WARD| SALESMAN|       7698|  1982-06-23|      1250|     500|      50|     50|     ADMIN|           CA|\n",
      "|  7934|  MILLER|    CLERK|       7788|  1982-01-02|      1300|     225|      50|     50|     ADMIN|           CA|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# MERGE: Suggests a sort-merge join strategy, which is often efficient for large DataFrames that are already sorted or can be efficiently sorted.\n",
    "#\n",
    "employee_df.join(dept_df.hint(\"merge\"), employee_df[\"emp_dept\"] == dept_df[\"dept_id\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7566721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|emp_id|emp_name|emp_role |emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|dept_id|dept_name |dept_location|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|7839  |KING    |PRESIDENT|NULL       |1981-11-17  |5000      |NULL    |10      |10     |ACCOUNTING|NY           |\n",
      "|7782  |CLARK   |MANAGER  |7839       |1983-09-06  |2450      |NULL    |10      |10     |ACCOUNTING|NY           |\n",
      "|7935  |ROBERT  |ANALYST  |7566       |1984-03-31  |1940      |225     |10      |10     |ACCOUNTING|NY           |\n",
      "|7566  |JONES   |MANAGER  |7839       |1981-02-04  |2975      |345     |20      |20     |RESEARCH  |TX           |\n",
      "|7788  |SCOTT   |ANALYST  |7566       |1984-12-17  |3000      |NULL    |20      |20     |RESEARCH  |TX           |\n",
      "|7369  |SMITH   |CLERK    |7902       |1980-04-04  |800       |NULL    |20      |20     |RESEARCH  |TX           |\n",
      "|7876  |ADAMS   |CLERK    |7788       |1980-11-14  |1100      |100     |20      |20     |RESEARCH  |TX           |\n",
      "|7698  |BLAKE   |MANAGER  |7839       |1981-01-05  |2850      |125     |30      |30     |SALES     |CH           |\n",
      "|7499  |ALLEN   |SALESMAN |7698       |1981-05-05  |1600      |300     |30      |30     |SALES     |CH           |\n",
      "|7654  |MARTIN  |SALESMAN |7698       |1981-07-21  |1250      |1400    |30      |30     |SALES     |CH           |\n",
      "|7902  |FORD    |ANALYST  |7566       |1981-11-27  |3000      |NULL    |40      |40     |OPERATIONS|TX           |\n",
      "|7844  |TURNER  |SALESMAN |7698       |1982-09-08  |1500      |0       |40      |40     |OPERATIONS|TX           |\n",
      "|7521  |WARD    |SALESMAN |7698       |1982-06-23  |1250      |500     |50      |50     |ADMIN     |CA           |\n",
      "|7934  |MILLER  |CLERK    |7788       |1982-01-02  |1300      |225     |50      |50     |ADMIN     |CA           |\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# SHUFFLE\\_MERGE: Similar to MERGE but explicitly suggests shuffling both DataFrames before performing a sort-merge join.\n",
    "#\n",
    "\n",
    "employee_df.join(dept_df.hint(\"shuffle_merge\"), employee_df[\"emp_dept\"] == dept_df[\"dept_id\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b87bbdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|emp_id|emp_name|emp_role |emp_manager|emp_hiredate|emp_salary|emp_comm|emp_dept|dept_id|dept_name |dept_location|\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "|7566  |JONES   |MANAGER  |7839       |1981-02-04  |2975      |345     |20      |20     |RESEARCH  |TX           |\n",
      "|7788  |SCOTT   |ANALYST  |7566       |1984-12-17  |3000      |NULL    |20      |20     |RESEARCH  |TX           |\n",
      "|7902  |FORD    |ANALYST  |7566       |1981-11-27  |3000      |NULL    |40      |40     |OPERATIONS|TX           |\n",
      "|7369  |SMITH   |CLERK    |7902       |1980-04-04  |800       |NULL    |20      |20     |RESEARCH  |TX           |\n",
      "|7844  |TURNER  |SALESMAN |7698       |1982-09-08  |1500      |0       |40      |40     |OPERATIONS|TX           |\n",
      "|7876  |ADAMS   |CLERK    |7788       |1980-11-14  |1100      |100     |20      |20     |RESEARCH  |TX           |\n",
      "|7839  |KING    |PRESIDENT|NULL       |1981-11-17  |5000      |NULL    |10      |10     |ACCOUNTING|NY           |\n",
      "|7782  |CLARK   |MANAGER  |7839       |1983-09-06  |2450      |NULL    |10      |10     |ACCOUNTING|NY           |\n",
      "|7935  |ROBERT  |ANALYST  |7566       |1984-03-31  |1940      |225     |10      |10     |ACCOUNTING|NY           |\n",
      "|7521  |WARD    |SALESMAN |7698       |1982-06-23  |1250      |500     |50      |50     |ADMIN     |CA           |\n",
      "|7934  |MILLER  |CLERK    |7788       |1982-01-02  |1300      |225     |50      |50     |ADMIN     |CA           |\n",
      "|7698  |BLAKE   |MANAGER  |7839       |1981-01-05  |2850      |125     |30      |30     |SALES     |CH           |\n",
      "|7499  |ALLEN   |SALESMAN |7698       |1981-05-05  |1600      |300     |30      |30     |SALES     |CH           |\n",
      "|7654  |MARTIN  |SALESMAN |7698       |1981-07-21  |1250      |1400    |30      |30     |SALES     |CH           |\n",
      "+------+--------+---------+-----------+------------+----------+--------+--------+-------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# SHUFFLE\\_HASH: Suggests a shuffle hash join, where both DataFrames are shuffled and then joined using a hash table on each partition.\n",
    "#\n",
    "employee_df.join(dept_df.hint(\"shuffle_hash\"), employee_df[\"emp_dept\"] == dept_df[\"dept_id\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061740f",
   "metadata": {},
   "source": [
    "#### 2. Partitioning Hints: \n",
    "These hints help control the number of partitions and their distribution, impacting output file size and parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# COALESCE: Reduces the number of partitions to a specified number.\n",
    "#\n",
    "employee_df.hint(\"coalesce\", 10).write.parquet(\"output_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5557ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# REPARTITION: Repartitions the DataFrame to a specified number of partitions, optionally by specific columns.\n",
    "#\n",
    "employee_df.hint(\"repartition\", 10, \"column_name\").write.parquet(\"output_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# REPARTITION\\_BY\\_RANGE: Repartitions the DataFrame by range based on specified columns, ensuring data within a range is in the same partition.\n",
    "#\n",
    "employee_df.hint(\"repartition_by_range\", \"column_name\").write.parquet(\"output_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# REBALANCE: Rebalances the partitions to achieve a more even distribution of data size across partitions.\n",
    "#\n",
    "employee_df.hint(\"rebalance\").write.parquet(\"output_path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
