{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    "    .set(\"spark.hadoop.fs.defaultFS\", \"s3a://defaultfs/\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-deltalake').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark\n",
    "\n",
    "#\n",
    "# \n",
    "#\n",
    "def display(df):\n",
    "    df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## Delete Existing Delta Table\n",
    "aws --endpoint-url http://minio.sandbox.net:9010 s3 rm s3://defaultfs/deltalake/peoples --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcde8f",
   "metadata": {},
   "source": [
    "#### Create Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"firstName\", StringType(), True),\n",
    "  StructField(\"middleName\", StringType(), True),\n",
    "  StructField(\"lastName\", StringType(), True),\n",
    "  StructField(\"gender\", StringType(), True),\n",
    "  StructField(\"birthDate\", TimestampType(), True),\n",
    "  StructField(\"ssn\", StringType(), True),\n",
    "  StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"s3a://datasets/peoples.csv\")\n",
    "df.printSchema()\n",
    "#\n",
    "display(df)\n",
    "\n",
    "\n",
    "# Save as delta table in S3\n",
    "df.write.format('delta').save('/deltalake/peoples')\n",
    "\n",
    "# Create the table if it does not exist. Otherwise, replace the existing table.\n",
    "#df.writeTo(\"spark_catalog.default.peoples\").createOrReplace()\n",
    "\n",
    "# If you know the table does not already exist, you can call this instead:\n",
    "#df.write.saveAsTable(\"spark_catalog.default.peoples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9839f68d",
   "metadata": {},
   "source": [
    "#### Python Create Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"peoples\") \\\n",
    "    .addColumn(\"id\", \"INT\") \\\n",
    "    .addColumn(\"firstName\", \"STRING\") \\\n",
    "    .addColumn(\"middleName\", \"STRING\") \\\n",
    "    .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
    "    .addColumn(\"gender\", \"STRING\") \\\n",
    "    .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
    "    .addColumn(\"ssn\", \"STRING\") \\\n",
    "    .addColumn(\"salary\", \"INT\") \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72df532",
   "metadata": {},
   "source": [
    "#### Upsert to a Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from datetime import date\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"firstName\", StringType(), True),\n",
    "  StructField(\"middleName\", StringType(), True),\n",
    "  StructField(\"lastName\", StringType(), True),\n",
    "  StructField(\"gender\", StringType(), True),\n",
    "  StructField(\"birthDate\", DateType(), True),\n",
    "  StructField(\"ssn\", StringType(), True),\n",
    "  StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "  (9999998, 'Billy', 'Tommie', 'Luppitt', 'M', date.fromisoformat('1992-09-17'), '953-38-9452', 55250),\n",
    "  (9999999, 'Elias', 'Cyril', 'Leadbetter', 'M', date.fromisoformat('1984-05-22'), '906-51-2137', 48500),\n",
    "  (10000000, 'Joshua', 'Chas', 'Broggio', 'M', date.fromisoformat('1968-07-22'), '988-61-6247', 90000),\n",
    "  (20000001, 'John', '', 'Doe', 'M', date.fromisoformat('1978-01-14'), '345-67-8901', 55500),\n",
    "  (20000002, 'Mary', '', 'Smith', 'F', date.fromisoformat('1982-10-29'), '456-78-9012', 98250),\n",
    "  (20000003, 'Jane', '', 'Doe', 'F', date.fromisoformat('1981-06-25'), '567-89-0123', 89900)\n",
    "]\n",
    "\n",
    "people_10m_updates = spark.createDataFrame(data, schema)\n",
    "people_10m_updates.createOrReplaceTempView(\"people_10m_updates\")\n",
    "\n",
    "# ...\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "\n",
    "(deltaTable.alias(\"people_10m\")\n",
    "  .merge(\n",
    "    people_10m_updates.alias(\"people_10m_updates\"),\n",
    "    \"people_10m.id = people_10m_updates.id\"\n",
    "  )\n",
    "  .whenMatchedUpdateAll()\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load(\"/deltalake/peoples\")\n",
    "df_filtered = df.filter(df[\"id\"] >= 9999998)\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b4009",
   "metadata": {},
   "source": [
    "#### Read a Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.format('delta').load(\"/deltalake/peoples\")\n",
    "display(people_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a365c",
   "metadata": {},
   "source": [
    "#### Write to a Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.mode(\"append\").saveAsTable(\"main.default.people_10m\")\n",
    "\n",
    "# Save as delta table\n",
    "df.write.format('delta').mode('append').save('/deltalake/delta-table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b866902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.mode(\"overwrite\").saveAsTable(\"main.default.people_10m\")\n",
    "\n",
    "# Save as delta table\n",
    "df.write.format('delta').mode('overwrite').save('/deltalake/delta-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28a7fe",
   "metadata": {},
   "source": [
    "#### Update Deltatable Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c60ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "\n",
    "# Declare the predicate by using a SQL-formatted string.\n",
    "deltaTable.update(\n",
    "  condition = \"gender = 'F'\",\n",
    "  set = { \"gender\": \"'Female'\" }\n",
    ")\n",
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.update(\n",
    "  condition = col('gender') == 'M',\n",
    "  set = { 'gender': lit('Male') }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977bdf31",
   "metadata": {},
   "source": [
    "#### Delete Rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158912ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "\n",
    "# Declare the predicate by using a SQL-formatted string.\n",
    "deltaTable.delete(\"birthDate < '1955-01-01'\")\n",
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.delete(col('birthDate') < '1960-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb1e12",
   "metadata": {},
   "source": [
    "#### Display table history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "display(deltaTable.history())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e30a44",
   "metadata": {},
   "source": [
    "#### overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as delta table\n",
    "df.write.format('delta').mode('overwrite').save('/deltalake/delta-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a334a43",
   "metadata": {},
   "source": [
    "#### Time Travle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version 1\n",
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "deltaHistory = deltaTable.history()\n",
    "\n",
    "display(deltaHistory.where(\"version == 0\"))\n",
    "# Or:\n",
    "display(deltaHistory.where(\"timestamp == '2024-05-15T22:43:15.000+00:00'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').option('versionAsOf', 0).load(\"/deltalake/peoples\")\n",
    "# Or: 2025-10-14 18:39:41\n",
    "#df = spark.read.format('delta').option('timestampAsOf', '2025-10-14T18:45:03.000+00:00').load(\"/deltalake/peoples\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df61a92",
   "metadata": {},
   "source": [
    "#### Display table history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b27616",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "print(\"######## Describe history for the table ######\")\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e1875",
   "metadata": {},
   "source": [
    "#### Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "print(\"######## Vacuum the table ########\")\n",
    "deltaTable.vacuum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791e50d",
   "metadata": {},
   "source": [
    "#### Describe details for the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"######## Describe details for the table ######\")\n",
    "deltaTable.detail().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d392faa",
   "metadata": {},
   "source": [
    "#### Generating manifest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate manifest\n",
    "print(\"######## Generating manifest ######\")\n",
    "deltaTable.generate(\"SYMLINK_FORMAT_MANIFEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ae7b11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### SQL Vacuum #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(path='s3a://defaultfs/deltalake/peoples')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Vacuum\n",
    "print(\"####### SQL Vacuum #######\")\n",
    "spark.sql(\"VACUUM '%s' RETAIN 169 HOURS\" % (\"/deltalake/peoples\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8711592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### SQL Describe History ########\n",
      "[Row(version=5, timestamp=datetime.datetime(2025, 10, 14, 18, 57, 41), userId=None, userName=None, operation='DELETE', operationParameters={'predicate': '[\"(birthDate#6062 < 1960-01-01 00:00:00)\"]'}, job=None, notebook=None, clusterId=None, readVersion=4, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '1', 'executionTimeMs': '1316', 'numDeletionVectorsRemoved': '0', 'numRemovedFiles': '1', 'rewriteTimeMs': '197', 'numRemovedBytes': '44629', 'scanTimeMs': '1119', 'numCopiedRows': '833', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numDeletedRows': '104', 'numAddedBytes': '40093'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=4, timestamp=datetime.datetime(2025, 10, 14, 18, 57, 39), userId=None, userName=None, operation='DELETE', operationParameters={'predicate': '[\"(birthDate#6062 < 1955-01-01 00:00:00)\"]'}, job=None, notebook=None, clusterId=None, readVersion=3, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '1', 'executionTimeMs': '1571', 'numDeletionVectorsRemoved': '0', 'numRemovedFiles': '1', 'rewriteTimeMs': '195', 'numRemovedBytes': '47341', 'scanTimeMs': '1375', 'numCopiedRows': '937', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numDeletedRows': '63', 'numAddedBytes': '44629'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=3, timestamp=datetime.datetime(2025, 10, 14, 18, 55, 38), userId=None, userName=None, operation='UPDATE', operationParameters={'predicate': '[\"(gender#3745 = M)\"]'}, job=None, notebook=None, clusterId=None, readVersion=2, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '4', 'executionTimeMs': '1564', 'numDeletionVectorsRemoved': '0', 'numUpdatedRows': '4', 'numRemovedFiles': '4', 'rewriteTimeMs': '197', 'numRemovedBytes': '8944', 'scanTimeMs': '1366', 'numCopiedRows': '0', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numAddedBytes': '9028'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=2, timestamp=datetime.datetime(2025, 10, 14, 18, 55, 35), userId=None, userName=None, operation='UPDATE', operationParameters={'predicate': '[\"(gender#3745 = F)\"]'}, job=None, notebook=None, clusterId=None, readVersion=1, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '3', 'executionTimeMs': '848', 'numDeletionVectorsRemoved': '0', 'numUpdatedRows': '1002', 'numRemovedFiles': '3', 'rewriteTimeMs': '244', 'numRemovedBytes': '51672', 'scanTimeMs': '603', 'numCopiedRows': '0', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numAddedBytes': '51778'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=1, timestamp=datetime.datetime(2025, 10, 14, 18, 45, 3), userId=None, userName=None, operation='MERGE', operationParameters={'matchedPredicates': '[{\"actionType\":\"update\"}]', 'predicate': '[\"(id#962 = id#946)\"]', 'notMatchedBySourcePredicates': '[]', 'notMatchedPredicates': '[{\"actionType\":\"insert\"}]'}, job=None, notebook=None, clusterId=None, readVersion=0, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numOutputRows': '6', 'numTargetBytesAdded': '13311', 'numTargetRowsInserted': '6', 'numTargetFilesAdded': '6', 'materializeSourceTimeMs': '1244', 'numTargetRowsMatchedDeleted': '0', 'numTargetFilesRemoved': '0', 'numTargetRowsMatchedUpdated': '0', 'executionTimeMs': '4187', 'numTargetDeletionVectorsUpdated': '0', 'numTargetRowsCopied': '0', 'rewriteTimeMs': '333', 'numTargetRowsUpdated': '0', 'numTargetDeletionVectorsRemoved': '0', 'numTargetRowsDeleted': '0', 'scanTimeMs': '2583', 'numSourceRows': '6', 'numTargetDeletionVectorsAdded': '0', 'numTargetChangeFilesAdded': '0', 'numTargetRowsNotMatchedBySourceUpdated': '0', 'numTargetRowsNotMatchedBySourceDeleted': '0', 'numTargetBytesRemoved': '0'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=0, timestamp=datetime.datetime(2025, 10, 14, 18, 39, 41), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'ErrorIfExists', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=None, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={'numOutputRows': '1000', 'numOutputBytes': '47305', 'numFiles': '1'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2')]\n"
     ]
    }
   ],
   "source": [
    "# SQL describe history\n",
    "print(\"####### SQL Describe History ########\")\n",
    "print(spark.sql(\"DESCRIBE HISTORY delta.`%s`\" % (\"/deltalake/peoples\")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db409e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# cleanup\n",
    "shutil.rmtree(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3380ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000000.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000000.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000001.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000001.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000002.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000002.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000003.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000004.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000003.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000005.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000005.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000004.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/_commits/\n",
      "delete: s3://defaultfs/deltalake/peoples/_symlink_format_manifest/manifest\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/_last_vacuum_info\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-11fd8e1c-1e23-4700-9bee-a1ff901b4f4d-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-5906f97c-f7fb-40c4-ba2b-93e44493aa3e-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-69e6852d-5dca-4f49-9673-5d5dcaafb493-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-9233c194-d9a4-419d-ac8b-6d6919c71ee5-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-9063835a-cfc9-4bb8-a126-a6c10def2b9f-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-9c312557-1c2c-4da3-b79f-f92189939543-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00001-405380cb-78b8-4c50-b15c-50d6e90ff600-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00001-a9da52f3-335d-41f5-a9ab-d3717236d46b-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00001-5c544394-1a39-4219-b5f6-66ec1e842131-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00003-9806582f-47e1-4c43-948a-f159c49f369d-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00002-3f4d7a69-2463-4d5a-b98f-e5eb7a6d11f5-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00003-f8544bf5-bebd-4eca-8107-02bd42199eaf-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00002-2511c700-882a-4ad3-9118-1d2519a71422-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00002-a687a04b-d5e1-4f37-8332-9780e421eb22-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00006-2047fe82-8d72-4738-b20a-1331a6865fb1-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00007-7197d3aa-5726-4138-ad8f-65dc255313f6-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00005-2247308f-9efd-4da6-9c4e-ef7ad08f864f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "aws --endpoint-url http://minio.sandbox.net:9010 s3 rm s3://defaultfs/deltalake/peoples --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3db9d",
   "metadata": {},
   "source": [
    "#### Optimize a table\n",
    "After you have performed multiple changes to a table, you might have a lot of small files. To improve the speed of read queries, you can use the optimize operation to collapse small files into larger ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13894522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "#deltaTable = DeltaTable.forName(spark, \"main.default.people_10m\")\n",
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caeca96",
   "metadata": {},
   "source": [
    "#### Z-order by columns\n",
    "To improve read performance further, you can collocate related information in the same set of files by z-ordering. Delta Lake data-skipping algorithms use this collocation to dramatically reduce the amount of data that needs to be read. To z-order data, you specify the columns to order on in the z-order by operation. For example, to collocate by gender, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "#deltaTable = DeltaTable.forName(spark, \"main.default.people_10m\")\n",
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "deltaTable.optimize().executeZOrderBy(\"gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a521253",
   "metadata": {},
   "source": [
    "#### Clean up snapshots with VACUUM\n",
    "Delta Lake provides snapshot isolation for reads, which means that it is safe to run an optimize operation even while other users or jobs are querying the table. Eventually however, you should clean up old snapshots. You can do this by running the vacuum operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "#deltaTable = DeltaTable.forName(spark, \"main.default.people_10m\")\n",
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "deltaTable.vacuum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a17f1",
   "metadata": {},
   "source": [
    "#### How do I find the last commit's version in the Spark session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
