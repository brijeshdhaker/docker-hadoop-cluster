{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3d0c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/brijeshdhaker/.ivy2/cache\n",
      "The jars for the packages stored in: /home/brijeshdhaker/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-403b4638-e71a-40cb-a8cf-9a70ab9c9916;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.0.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.199 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.3.2 in local-m2-cache\n",
      "\tfound io.delta#delta-storage;3.3.2 in local-m2-cache\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in local-m2-cache\n",
      ":: resolution report :: resolve 199ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.199 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.3.2 from local-m2-cache in [default]\n",
      "\tio.delta#delta-storage;3.3.2 from local-m2-cache in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-403b4638-e71a-40cb-a8cf-9a70ab9c9916\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/5ms)\n",
      "25/10/21 23:09:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/21 23:09:02 ERROR SparkContext: Error initializing SparkContext.\n",
      "org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@vmware-ubuntu-25.04:44853\n",
      "\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/10/21 23:09:02 ERROR Utils: Uncaught exception in thread Thread-4\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.rpc.RpcEndpointRef.ask(Object, scala.reflect.ClassTag)\" because the return value of \"org.apache.spark.scheduler.local.LocalSchedulerBackend.localEndpoint()\" is null\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.org$apache$spark$scheduler$local$LocalSchedulerBackend$$stop(LocalSchedulerBackend.scala:173)\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:144)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:103)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:706)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/10/21 23:09:02 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@vmware-ubuntu-25.04:44853\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Adding AWS S3 Minio configs\u001b[39;00m\n\u001b[32m      8\u001b[39m sparkConf = (\n\u001b[32m      9\u001b[39m     SparkConf()\n\u001b[32m     10\u001b[39m     .set(\u001b[33m\"\u001b[39m\u001b[33mspark.jars.ivy\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m/home/brijeshdhaker/.ivy2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m#.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\u001b[39;00m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m spark = (\n\u001b[32m     28\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspark-deltalake\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m'\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m spark\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/context.py:203\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    201\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/context.py:296\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/pyspark/context.py:421\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/py4j/java_gateway.py:1587\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1581\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1583\u001b[39m     args_command +\\\n\u001b[32m   1584\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1586\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/env_python3_11_13/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@vmware-ubuntu-25.04:44853\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:301)\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adding AWS S3 Minio configs\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars.ivy\",\"/home/brijeshdhaker/.ivy2\")\n",
    "    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .set(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.0.0,io.delta:delta-spark_2.12:3.3.2\")\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    "    .set(\"spark.hadoop.fs.defaultFS\", \"s3a://defaultfs/\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio.sandbox.net:9010\")\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", \"pgm2H2bR7a5kMc5XCYdO\")\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", \"zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG\")\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    #.set(\"spark.eventLog.enabled\", \"true\")\n",
    "    #.set(\"spark.eventLog.dir\", \"file:///apps/var/logs/spark-events\")\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\").\n",
    "        appName('spark-deltalake').\n",
    "        config(conf=sparkConf).\n",
    "        getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark\n",
    "\n",
    "#\n",
    "# \n",
    "#\n",
    "def display(df):\n",
    "    df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "## Delete Existing Delta Table\n",
    "aws --endpoint-url http://minio.sandbox.net:9010 s3 rm s3://defaultfs/deltalake/peoples --recursive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebcde8f",
   "metadata": {},
   "source": [
    "#### Create Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"firstName\", StringType(), True),\n",
    "  StructField(\"middleName\", StringType(), True),\n",
    "  StructField(\"lastName\", StringType(), True),\n",
    "  StructField(\"gender\", StringType(), True),\n",
    "  StructField(\"birthDate\", TimestampType(), True),\n",
    "  StructField(\"ssn\", StringType(), True),\n",
    "  StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"s3a://datasets/peoples.csv\")\n",
    "df.printSchema()\n",
    "#\n",
    "display(df)\n",
    "\n",
    "\n",
    "# Save as delta table in S3\n",
    "df.write.format('delta').save('/deltalake/peoples')\n",
    "\n",
    "# Create the table if it does not exist. Otherwise, replace the existing table.\n",
    "#df.writeTo(\"spark_catalog.default.peoples\").createOrReplace()\n",
    "\n",
    "# If you know the table does not already exist, you can call this instead:\n",
    "#df.write.saveAsTable(\"spark_catalog.default.peoples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9839f68d",
   "metadata": {},
   "source": [
    "#### Python Create Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"peoples\") \\\n",
    "    .addColumn(\"id\", \"INT\") \\\n",
    "    .addColumn(\"firstName\", \"STRING\") \\\n",
    "    .addColumn(\"middleName\", \"STRING\") \\\n",
    "    .addColumn(\"lastName\", \"STRING\", comment = \"surname\") \\\n",
    "    .addColumn(\"gender\", \"STRING\") \\\n",
    "    .addColumn(\"birthDate\", \"TIMESTAMP\") \\\n",
    "    .addColumn(\"ssn\", \"STRING\") \\\n",
    "    .addColumn(\"salary\", \"INT\") \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72df532",
   "metadata": {},
   "source": [
    "#### Upsert to a Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from datetime import date\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"firstName\", StringType(), True),\n",
    "  StructField(\"middleName\", StringType(), True),\n",
    "  StructField(\"lastName\", StringType(), True),\n",
    "  StructField(\"gender\", StringType(), True),\n",
    "  StructField(\"birthDate\", DateType(), True),\n",
    "  StructField(\"ssn\", StringType(), True),\n",
    "  StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "  (9999998, 'Billy', 'Tommie', 'Luppitt', 'M', date.fromisoformat('1992-09-17'), '953-38-9452', 55250),\n",
    "  (9999999, 'Elias', 'Cyril', 'Leadbetter', 'M', date.fromisoformat('1984-05-22'), '906-51-2137', 48500),\n",
    "  (10000000, 'Joshua', 'Chas', 'Broggio', 'M', date.fromisoformat('1968-07-22'), '988-61-6247', 90000),\n",
    "  (20000001, 'John', '', 'Doe', 'M', date.fromisoformat('1978-01-14'), '345-67-8901', 55500),\n",
    "  (20000002, 'Mary', '', 'Smith', 'F', date.fromisoformat('1982-10-29'), '456-78-9012', 98250),\n",
    "  (20000003, 'Jane', '', 'Doe', 'F', date.fromisoformat('1981-06-25'), '567-89-0123', 89900)\n",
    "]\n",
    "\n",
    "people_10m_updates = spark.createDataFrame(data, schema)\n",
    "people_10m_updates.createOrReplaceTempView(\"people_10m_updates\")\n",
    "\n",
    "# ...\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "\n",
    "(deltaTable.alias(\"people_10m\")\n",
    "  .merge(\n",
    "    people_10m_updates.alias(\"people_10m_updates\"),\n",
    "    \"people_10m.id = people_10m_updates.id\"\n",
    "  )\n",
    "  .whenMatchedUpdateAll()\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').load(\"/deltalake/peoples\")\n",
    "df_filtered = df.filter(df[\"id\"] >= 9999998)\n",
    "display(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b4009",
   "metadata": {},
   "source": [
    "#### Read a Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.format('delta').load(\"/deltalake/peoples\")\n",
    "display(people_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a365c",
   "metadata": {},
   "source": [
    "#### Write to a Deltatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.mode(\"append\").saveAsTable(\"main.default.people_10m\")\n",
    "\n",
    "# Save as delta table\n",
    "df.write.format('delta').mode('append').save('/deltalake/delta-table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b866902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.mode(\"overwrite\").saveAsTable(\"main.default.people_10m\")\n",
    "\n",
    "# Save as delta table\n",
    "df.write.format('delta').mode('overwrite').save('/deltalake/delta-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28a7fe",
   "metadata": {},
   "source": [
    "#### Update Deltatable Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c60ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "\n",
    "# Declare the predicate by using a SQL-formatted string.\n",
    "deltaTable.update(\n",
    "  condition = \"gender = 'F'\",\n",
    "  set = { \"gender\": \"'Female'\" }\n",
    ")\n",
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.update(\n",
    "  condition = col('gender') == 'M',\n",
    "  set = { 'gender': lit('Male') }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977bdf31",
   "metadata": {},
   "source": [
    "#### Delete Rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158912ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "\n",
    "# Declare the predicate by using a SQL-formatted string.\n",
    "deltaTable.delete(\"birthDate < '1955-01-01'\")\n",
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.delete(col('birthDate') < '1960-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb1e12",
   "metadata": {},
   "source": [
    "#### Display table history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "display(deltaTable.history())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e30a44",
   "metadata": {},
   "source": [
    "#### overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as delta table\n",
    "df.write.format('delta').mode('overwrite').save('/deltalake/delta-table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a334a43",
   "metadata": {},
   "source": [
    "#### Time Travle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read version 1\n",
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/deltalake/peoples')\n",
    "deltaHistory = deltaTable.history()\n",
    "\n",
    "display(deltaHistory.where(\"version == 0\"))\n",
    "# Or:\n",
    "display(deltaHistory.where(\"timestamp == '2024-05-15T22:43:15.000+00:00'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').option('versionAsOf', 0).load(\"/deltalake/peoples\")\n",
    "# Or: 2025-10-14 18:39:41\n",
    "#df = spark.read.format('delta').option('timestampAsOf', '2025-10-14T18:45:03.000+00:00').load(\"/deltalake/peoples\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df61a92",
   "metadata": {},
   "source": [
    "#### Display table history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b27616",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "print(\"######## Describe history for the table ######\")\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e1875",
   "metadata": {},
   "source": [
    "#### Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "print(\"######## Vacuum the table ########\")\n",
    "deltaTable.vacuum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791e50d",
   "metadata": {},
   "source": [
    "#### Describe details for the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"######## Describe details for the table ######\")\n",
    "deltaTable.detail().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d392faa",
   "metadata": {},
   "source": [
    "#### Generating manifest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0cd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate manifest\n",
    "print(\"######## Generating manifest ######\")\n",
    "deltaTable.generate(\"SYMLINK_FORMAT_MANIFEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ae7b11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### SQL Vacuum #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 0 files and directories in a total of 1 directories.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(path='s3a://defaultfs/deltalake/peoples')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Vacuum\n",
    "print(\"####### SQL Vacuum #######\")\n",
    "spark.sql(\"VACUUM '%s' RETAIN 169 HOURS\" % (\"/deltalake/peoples\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8711592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### SQL Describe History ########\n",
      "[Row(version=5, timestamp=datetime.datetime(2025, 10, 14, 18, 57, 41), userId=None, userName=None, operation='DELETE', operationParameters={'predicate': '[\"(birthDate#6062 < 1960-01-01 00:00:00)\"]'}, job=None, notebook=None, clusterId=None, readVersion=4, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '1', 'executionTimeMs': '1316', 'numDeletionVectorsRemoved': '0', 'numRemovedFiles': '1', 'rewriteTimeMs': '197', 'numRemovedBytes': '44629', 'scanTimeMs': '1119', 'numCopiedRows': '833', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numDeletedRows': '104', 'numAddedBytes': '40093'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=4, timestamp=datetime.datetime(2025, 10, 14, 18, 57, 39), userId=None, userName=None, operation='DELETE', operationParameters={'predicate': '[\"(birthDate#6062 < 1955-01-01 00:00:00)\"]'}, job=None, notebook=None, clusterId=None, readVersion=3, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '1', 'executionTimeMs': '1571', 'numDeletionVectorsRemoved': '0', 'numRemovedFiles': '1', 'rewriteTimeMs': '195', 'numRemovedBytes': '47341', 'scanTimeMs': '1375', 'numCopiedRows': '937', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numDeletedRows': '63', 'numAddedBytes': '44629'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=3, timestamp=datetime.datetime(2025, 10, 14, 18, 55, 38), userId=None, userName=None, operation='UPDATE', operationParameters={'predicate': '[\"(gender#3745 = M)\"]'}, job=None, notebook=None, clusterId=None, readVersion=2, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '4', 'executionTimeMs': '1564', 'numDeletionVectorsRemoved': '0', 'numUpdatedRows': '4', 'numRemovedFiles': '4', 'rewriteTimeMs': '197', 'numRemovedBytes': '8944', 'scanTimeMs': '1366', 'numCopiedRows': '0', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numAddedBytes': '9028'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=2, timestamp=datetime.datetime(2025, 10, 14, 18, 55, 35), userId=None, userName=None, operation='UPDATE', operationParameters={'predicate': '[\"(gender#3745 = F)\"]'}, job=None, notebook=None, clusterId=None, readVersion=1, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numDeletionVectorsUpdated': '0', 'numAddedFiles': '3', 'executionTimeMs': '848', 'numDeletionVectorsRemoved': '0', 'numUpdatedRows': '1002', 'numRemovedFiles': '3', 'rewriteTimeMs': '244', 'numRemovedBytes': '51672', 'scanTimeMs': '603', 'numCopiedRows': '0', 'numDeletionVectorsAdded': '0', 'numAddedChangeFiles': '0', 'numAddedBytes': '51778'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=1, timestamp=datetime.datetime(2025, 10, 14, 18, 45, 3), userId=None, userName=None, operation='MERGE', operationParameters={'matchedPredicates': '[{\"actionType\":\"update\"}]', 'predicate': '[\"(id#962 = id#946)\"]', 'notMatchedBySourcePredicates': '[]', 'notMatchedPredicates': '[{\"actionType\":\"insert\"}]'}, job=None, notebook=None, clusterId=None, readVersion=0, isolationLevel='Serializable', isBlindAppend=False, operationMetrics={'numOutputRows': '6', 'numTargetBytesAdded': '13311', 'numTargetRowsInserted': '6', 'numTargetFilesAdded': '6', 'materializeSourceTimeMs': '1244', 'numTargetRowsMatchedDeleted': '0', 'numTargetFilesRemoved': '0', 'numTargetRowsMatchedUpdated': '0', 'executionTimeMs': '4187', 'numTargetDeletionVectorsUpdated': '0', 'numTargetRowsCopied': '0', 'rewriteTimeMs': '333', 'numTargetRowsUpdated': '0', 'numTargetDeletionVectorsRemoved': '0', 'numTargetRowsDeleted': '0', 'scanTimeMs': '2583', 'numSourceRows': '6', 'numTargetDeletionVectorsAdded': '0', 'numTargetChangeFilesAdded': '0', 'numTargetRowsNotMatchedBySourceUpdated': '0', 'numTargetRowsNotMatchedBySourceDeleted': '0', 'numTargetBytesRemoved': '0'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2'), Row(version=0, timestamp=datetime.datetime(2025, 10, 14, 18, 39, 41), userId=None, userName=None, operation='WRITE', operationParameters={'mode': 'ErrorIfExists', 'partitionBy': '[]'}, job=None, notebook=None, clusterId=None, readVersion=None, isolationLevel='Serializable', isBlindAppend=True, operationMetrics={'numOutputRows': '1000', 'numOutputBytes': '47305', 'numFiles': '1'}, userMetadata=None, engineInfo='Apache-Spark/3.5.3 Delta-Lake/3.3.2')]\n"
     ]
    }
   ],
   "source": [
    "# SQL describe history\n",
    "print(\"####### SQL Describe History ########\")\n",
    "print(spark.sql(\"DESCRIBE HISTORY delta.`%s`\" % (\"/deltalake/peoples\")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db409e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# cleanup\n",
    "shutil.rmtree(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3380ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000000.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000000.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000001.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000001.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000002.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000002.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000003.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000004.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000003.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000005.crc\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000005.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/00000000000000000004.json\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/_commits/\n",
      "delete: s3://defaultfs/deltalake/peoples/_symlink_format_manifest/manifest\n",
      "delete: s3://defaultfs/deltalake/peoples/_delta_log/_last_vacuum_info\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-11fd8e1c-1e23-4700-9bee-a1ff901b4f4d-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-5906f97c-f7fb-40c4-ba2b-93e44493aa3e-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-69e6852d-5dca-4f49-9673-5d5dcaafb493-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-9233c194-d9a4-419d-ac8b-6d6919c71ee5-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-9063835a-cfc9-4bb8-a126-a6c10def2b9f-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00000-9c312557-1c2c-4da3-b79f-f92189939543-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00001-405380cb-78b8-4c50-b15c-50d6e90ff600-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00001-a9da52f3-335d-41f5-a9ab-d3717236d46b-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00001-5c544394-1a39-4219-b5f6-66ec1e842131-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00003-9806582f-47e1-4c43-948a-f159c49f369d-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00002-3f4d7a69-2463-4d5a-b98f-e5eb7a6d11f5-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00003-f8544bf5-bebd-4eca-8107-02bd42199eaf-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00002-2511c700-882a-4ad3-9118-1d2519a71422-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00002-a687a04b-d5e1-4f37-8332-9780e421eb22-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00006-2047fe82-8d72-4738-b20a-1331a6865fb1-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00007-7197d3aa-5726-4138-ad8f-65dc255313f6-c000.snappy.parquet\n",
      "delete: s3://defaultfs/deltalake/peoples/part-00005-2247308f-9efd-4da6-9c4e-ef7ad08f864f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "aws --endpoint-url http://minio.sandbox.net:9010 s3 rm s3://defaultfs/deltalake/peoples --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3db9d",
   "metadata": {},
   "source": [
    "#### Optimize a table\n",
    "After you have performed multiple changes to a table, you might have a lot of small files. To improve the speed of read queries, you can use the optimize operation to collapse small files into larger ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13894522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "#deltaTable = DeltaTable.forName(spark, \"main.default.people_10m\")\n",
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caeca96",
   "metadata": {},
   "source": [
    "#### Z-order by columns\n",
    "To improve read performance further, you can collocate related information in the same set of files by z-ordering. Delta Lake data-skipping algorithms use this collocation to dramatically reduce the amount of data that needs to be read. To z-order data, you specify the columns to order on in the z-order by operation. For example, to collocate by gender, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "#deltaTable = DeltaTable.forName(spark, \"main.default.people_10m\")\n",
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "deltaTable.optimize().executeZOrderBy(\"gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a521253",
   "metadata": {},
   "source": [
    "#### Clean up snapshots with VACUUM\n",
    "Delta Lake provides snapshot isolation for reads, which means that it is safe to run an optimize operation even while other users or jobs are querying the table. Eventually however, you should clean up old snapshots. You can do this by running the vacuum operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "#deltaTable = DeltaTable.forName(spark, \"main.default.people_10m\")\n",
    "deltaTable = DeltaTable.forPath(spark, \"/deltalake/peoples\")\n",
    "deltaTable.vacuum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a17f1",
   "metadata": {},
   "source": [
    "#### How do I find the last commit's version in the Spark session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python3_11_13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
