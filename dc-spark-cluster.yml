#
# docker volume create --name sandbox_spark_313 --opt type=none --opt device=/opt/spark-3.1.3 --opt o=bind
# docker volume create --name sandbox_spark_340 --opt type=none --opt device=/opt/spark-3.4.0 --opt o=bind
#

# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.9'
services:
  #
  # HDFS & YARN Sandbox
  #
  namenode:
    image: brijeshdhaker/hadoop-namenode:3.3.4
    container_name: namenode
    hostname: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - sandbox_hadoop_334_dfs:/hadoop/dfs
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
    environment:
      - CLUSTER_NAME=docker-sandbox
    env_file:
      - envs/docker_hadoop.env

  #
  datanode:
    image: brijeshdhaker/hadoop-datanode:3.3.4
    container_name: datanode
    hostname: datanode
    ports:
      - "9864:9864"
      - "9866:9866"
    restart: always
    volumes:
      - sandbox_hadoop_334_dfs:/hadoop/dfs
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - envs/docker_hadoop.env

  #
  #
  #
  spark-master:
    image: brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_312:/opt/hive
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
      - WEBUI_PORT= 8080
    env_file:
      - envs/docker_spark312.env
  #
  #
  #
  spark-worker-a:
    image: brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-worker-a
    hostname: spark-worker-a
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    env_file:
      - envs/docker_spark312.env
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKLOAD=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-a
      - SPARK_WORKER_WEBUI_PORT=8081
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_312:/opt/hive

  #
  #
  #
  spark-worker-b:
    image: brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-worker-b
    hostname: spark-worker-b
    ports:
      - "8082:8082"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
      - SPARK_WORKER_WEBUI_PORT=8082
    env_file:
      - envs/docker_spark312.env
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_312:/opt/hive

  #
  #
  #
  spark-historyserver:
    image: brijeshdhaker/spark:3.1.2-standalone
    container_name: spark-historyserver
    hostname: spark-historyserver
    environment:
      SPARK_WORKLOAD: HistoryServer
    depends_on:
      - namenode
      - datanode
    ports:
      - "18080:18080"
    env_file:
      - envs/docker_spark312.env
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - ./conf/spark/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

  #
  # Zeppelin Notebook
  #
  zeppelin:
    image: apache/zeppelin:0.10.1
    container_name: zeppelin
    hostname: zeppelin
    env_file:
      - envs/docker_zeppelin.env
    ports:
      - "9080:8080"
      - "4040:4040"
    volumes:
      - sandbox_host_path:/apps/hostpath
      - sandbox_base_path:/apps/sandbox
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_312:/opt/hive
      - sandbox_hbase_249:/opt/hbase
      - sandbox_hbase_117:/opt/hbase-client
      - sandbox_spark_312:/opt/spark
      - sandbox_zeppelin:/opt/zeppelin
      - sandbox_zeppelin_notebook:/opt/notebook


#
volumes:
  sandbox_host_path:
    external: true
  sandbox_base_path:
    external: true
  sandbox_hadoop_334_dfs:
    external: true
  sandbox_zeppelin:
    external: true
  sandbox_zeppelin_notebook:
    external: true
  sandbox_hadoop_334:
    external: true
  sandbox_hbase_249:
    external: true
  sandbox_hbase_117:
    external: true
  sandbox_hive_312:
    external: true
  sandbox_spark_312:
    external: true

#
networks:
  default:
    external: true
    driver: bridge
    name: sandbox.net
