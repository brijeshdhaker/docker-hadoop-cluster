# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

#
spark.master                                                spark://spark-master.sandbox.net:7077
spark.jars.ivy                                              /apps/.ivy2
spark.jars.packages                                         org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.1,org.apache.iceberg:iceberg-aws:1.4.3,org.apache.spark:spark-hadoop-cloud_2.12:3.4.1

#
spark.driver.memory                                         1024m
spark.driver.cores                                          1
spark.executor.instances                                    2
spark.executor.memory                                       1024m
spark.executor.cores                                        2

#
spark.sql.extensions                                        org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog                             org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type                        hive

#
spark.sql.catalog.hadoop_prod                               org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.hadoop_prod.type                          hadoop
spark.sql.catalog.hadoop_prod.warehouse                     s3a://warehouse-hadoop/

#
spark.sql.catalog.demo                                      org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.demo.type                                 hadoop
spark.sql.catalog.demo.warehouse                            s3://warehouse-demo/
spark.sql.catalog.demo.io-impl                              org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.demo.s3.endpoint                          http://minio:9010

#
spark.sql.catalog.rest_prod                                 org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.rest_prod.type                            rest
spark.sql.catalog.rest_prod.uri                             http://iceberg-rest.sandbox.net:8181
spark.sql.catalog.rest_prod.io-impl                         org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.rest_prod.warehouse                       s3://warehouse-rest/
spark.sql.catalog.rest_prod.s3.endpoint                     http://minio:9010

#
spark.sql.catalog.data                                      org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.data.catalog-impl                         org.apache.iceberg.jdbc.JdbcCatalog
spark.sql.catalog.data.io-impl                              org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.data.warehouse                            s3://warehouse-data/
spark.sql.catalog.data.s3.endpoint                          http://minio:9010
spark.sql.catalog.data.uri                                  jdbc:mysql://mysqlserver.sandbox.net:3306/demo_catalog
spark.sql.catalog.data.jdbc.user                            mysqladmin
spark.sql.catalog.data.jdbc.password                        mysqladmin
spark.sql.catalog.data.jdbc.initialize                      true
spark.sql.catalog.data.jdbc.schema-version                  V1

#
spark.sql.catalog.mysql_prod                                org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.mysql_prod.catalog-impl                   org.apache.iceberg.jdbc.JdbcCatalog
spark.sql.catalog.mysql_prod.uri                            jdbc:mysql://mysqlserver.sandbox.net:3306/ICEBERG_REST_CATALOG
spark.sql.catalog.mysql_prod.jdbc.user                      mysqladmin
spark.sql.catalog.mysql_prod.jdbc.password                  mysqladmin
spark.sql.catalog.mysql_prod.warehouse                      s3a://warehouse-jdbc/

#
spark.sql.defaultCatalog                                    hadoop_prod
spark.sql.catalogImplementation                             in-memory
spark.hadoop.hive.cli.print.header                          true

spark.eventLog.enabled                                      true
spark.eventLog.dir                                          /apps/var/logs/spark-events
spark.history.fs.logDirectory                               /apps/var/logs/spark-events

#
# spark.hadoop.fs.defaultFS                                   s3a://warehouse-hadoop/
spark.hadoop.fs.s3a.endpoint                                http://minio.sandbox.net:9010
spark.hadoop.fs.s3a.access.key                              pgm2H2bR7a5kMc5XCYdO
spark.hadoop.fs.s3a.secret.key                              zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG
spark.hadoop.fs.s3a.path.style.access                       true
spark.hadoop.fs.s3a.aws.credentials.provider                org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.impl                                    org.apache.hadoop.fs.s3a.S3AFileSystem

#
spark.driver.extraJavaOptions                               '-Divy.home=/apps/.ivy2 -Djava.security.krb5.conf=/etc/krb5.conf -DAWS_ACCESS_KEY_ID=pgm2H2bR7a5kMc5XCYdO -DAWS_SECRET_ACCESS_KEY=zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG'
spark.executor.extraJavaOptions                             '-Divy.home=/apps/.ivy2 -Djava.security.krb5.conf=/etc/krb5.conf -DAWS_ACCESS_KEY_ID=pgm2H2bR7a5kMc5XCYdO -DAWS_SECRET_ACCESS_KEY=zjd8T0hXFGtfemVQ6AH3yBAPASJNXNbVSx5iddqG'
