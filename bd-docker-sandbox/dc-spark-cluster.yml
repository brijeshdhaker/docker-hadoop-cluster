# docker-compose build
# docker-compose up -d
# docker-compose scale nodemanager=X; # X=integer number --> allows to add more nodes to the hadoop cluster for testing

version: '3.9'

include:
  - dc-hadoop-cluster.yaml

services:
  #
  #
  #
  spark-master:
    image: docker.io/brijeshdhaker/spark-standalon:3.5.0
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - ./conf/kerberos/krb5.conf:/etc/krb5.conf
      - ./conf/hadoop/client:/opt/hadoop/etc/hadoop
      - ./conf/spark/conf:/opt/spark/conf
    env_file:
      - ./envs/docker_clients.env
    environment:
      - SPARK_MASTER_LOG=/apps/var/log/spark/spark-master.out
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_DRIVER_CORES=1
      - SPARK_DRIVER_MEMORY=1G

  #
  #
  #
  spark-worker-a:
    image: docker.io/brijeshdhaker/spark-standalon:3.5.0
    container_name: spark-worker-a
    hostname: spark-worker-a
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    env_file:
      - ./envs/docker_clients.env
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-a
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_LOG=/apps/var/log/spark/spark-worker-a.out
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - ./conf/kerberos/krb5.conf:/etc/krb5.conf
      - ./conf/hadoop/client:/opt/hadoop/etc/hadoop
      - ./conf/spark/conf:/opt/spark/conf


  #
  #
  #
  spark-worker-b:
    image: docker.io/brijeshdhaker/spark-standalon:3.5.0
    container_name: spark-worker-b
    hostname: spark-worker-b
    ports:
      - "8082:8082"
    depends_on:
      - spark-master
    env_file:
      - ./envs/docker_clients.env
    environment:
      - SPARK_WORKLOAD=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_LOCAL_IP=spark-worker-b
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_WORKER_LOG=/apps/var/log/spark/spark-worker-b.out
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - ./conf/kerberos/krb5.conf:/etc/krb5.conf
      - ./conf/hadoop/client:/opt/hadoop/etc/hadoop
      - ./conf/spark/conf:/opt/spark/conf

  #
  # Spark History Server
  #
  sparkhistory:
    image: brijeshdhaker/spark-standalon:3.5.0
    container_name: sparkhistory
    hostname: sparkhistory.sandbox.net
    healthcheck:
      test: curl -f http://sparkhistory.sandbox.net:18080 || exit 1
      retries: 20
      interval: 10s
    environment:
      SPARK_WORKLOAD: HistoryServer
      SERVICE_PRECONDITION: "namenode.sandbox.net:9000 namenode.sandbox.net:9870"
    env_file:
      - ./envs/docker_clients.env
    ports:
      - "18080:18080"
    depends_on:
      namenode:
        condition: service_healthy
      datanode:
        condition: service_healthy
    volumes:
      - sandbox_apps_path:/apps
      - sandbox_hadoop_334:/opt/hadoop
      - sandbox_hive_313:/opt/hive
      - ./conf/kerberos/krb5.conf:/etc/krb5.conf
      - ./conf/hadoop/client:/opt/hadoop/etc/hadoop
      - ./conf/spark/conf:/opt/spark/conf


#
volumes:
  sandbox_apps_path:
    external: true
  #
  sandbox_hadoop_334:
    external: true
  sandbox_hive_313:
    external: true
  #
  sandbox_spark_350:
    external: true

#
networks:
  default:
    external: true
    driver: bridge
    name: sandbox.net
